
@techreport{redmonYOLOv3IncrementalImprovement2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	number = {arXiv:1804.02767},
	urldate = {2023-03-12},
	institution = {arXiv},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv:1804.02767 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech Report},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\3348XYSL\\Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\E9GME244\\1804.html:text/html},
}

@inproceedings{tasnimROSbasedVoiceControlled2022,
	address = {Shanghai, China},
	title = {A {ROS}-based {Voice} {Controlled} {Robotic} {Arm} for {Automatic} {Segregation} of {Medical} {Waste} {Using} {YOLOv3}},
	isbn = {978-1-66546-674-5},
	url = {https://ieeexplore.ieee.org/document/9790152/},
	doi = {10.1109/ICCCR54399.2022.9790152},
	urldate = {2023-03-13},
	booktitle = {2022 2nd {International} {Conference} on {Computer}, {Control} and {Robotics} ({ICCCR})},
	publisher = {IEEE},
	author = {Tasnim, Samiha and Hasan, Md. Zahid and Ahmed, Touhid and Hasan, Md. Tanvir and Uddin, Ferdousi Jamal and Farah, Tanjila},
	month = mar,
	year = {2022},
	pages = {81--85},
}

@misc{PapersCodePedestrian,
	title = {Papers with {Code} - {Pedestrian} {Detection}},
	url = {https://paperswithcode.com/task/pedestrian-detection},
	abstract = {Pedestrian detection is the task of detecting pedestrians from a camera.

Further state-of-the-art results (e.g. on the KITTI dataset) can be found at [3D Object Detection](https://paperswithcode.com/task/object-detection).

{\textless}span style="color:grey; opacity: 0.6"{\textgreater}( Image credit: [High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection](https://github.com/liuwei16/CSP) ){\textless}/span{\textgreater}},
	language = {en},
	urldate = {2023-03-13},
	file = {Snapshot:H\:\\Zotero\\storage\\IAAU9FC5\\pedestrian-detection.html:text/html},
}

@inproceedings{wangPedestrianDetectionCrowded2016,
	title = {Pedestrian detection in crowded scenes via scale and occlusion analysis},
	doi = {10.1109/ICIP.2016.7532550},
	abstract = {Despite significant progress in pedestrian detection has been made in recent years, detecting pedestrians in crowded scenes remains a challenging problem. In this paper, we propose to use visual contexts based on scale and occlusion cues from detections at proximity to better detect pedestrians for surveillance applications. Specifically, we first apply detectors based on full body and parts to generate initial detections. Scale prior at each image location is estimated using the cues provided by neighboring detections, and the confidence score of each detection is refined according to its consistency with the estimated scale prior. Local occlusion analysis is exploited in refining detection confidence scores which facilitates the final detection cluster based Non-Maximum Suppression. Experimental results on benchmark data sets show that the proposed algorithm performs favorably against the state-of-the-art methods.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Wang, Lu and Xu, Lisheng and Yang, Ming-Hsuan},
	month = sep,
	year = {2016},
	note = {ISSN: 2381-8549},
	keywords = {Detectors, Algorithm design and analysis, Benchmark testing, crowded scenes, Estimation, occlusion analysis, Optimization, Pedestrian detection, Positron emission tomography, scale prior, Visualization},
	pages = {1210--1214},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\5QV4SGI2\\7532550.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\VFUSHWA3\\Wang et al. - 2016 - Pedestrian detection in crowded scenes via scale a.pdf:application/pdf},
}

@inproceedings{liResearchLightweightPedestrian2022,
	title = {Research on {Lightweight} {Pedestrian} {Detection} {Model} in {Complex} {Background}},
	doi = {10.1109/MLCR57210.2022.00025},
	abstract = {Pedestrian detection under complex background is a hot research content in the field of Computer Vision. Pedestrian detection in a real and complex environment is extremely important for Human Action Recognition, Intelligent Transportation, Video Surveillance Security and other security aspects. This paper proposes a lightweight model based on the YOLO algorithm for pedestrian detection in a complex background. First, we build a YOLO v3 model based on Darknet-53. Then in order to simplify the pedestrian detection model and ensure the performance of the model, this paper adopts the method of convolution channel pruning to lighten the model. This method realizes rapid and accurate detection of pedestrians in complex backgrounds. In order to verify the performance of the proposed model, this paper constructs a scientific and reasonable small dataset sample based on the YOLO model. The dataset contains 1505 images, covering various complex scenes such as intersections with crowded people and traffic at night, and commercial streets with complex buildings. The test results show that for a complex background dataset with a small number of samples, the model's mAP reached 87.89\%, precision was 90.37\%, and recall was 77.05\%. In addition, this paper also compares the performance of YOLO v3 and YOLO v4-tiny models. The results show that the lightweight models proposed in this paper have strong robustness. It is feasible to apply this method to real-time and accurate detection of pedestrians in complex backgrounds.},
	booktitle = {2022 {International} {Conference} on {Machine} {Learning}, {Control}, and {Robotics} ({MLCR})},
	author = {Li, Jianjun and Han, Yu and Guo, Lixin and Hao, Weixun},
	month = oct,
	year = {2022},
	keywords = {Training, Computational modeling, complex backgrounds, Convolution, Data models, Lightweight model, Pedestrian Detection, Real-time systems, Transportation, Video surveillance, YOLO v3},
	pages = {91--95},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\L9UWCRJ8\\10037023.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\5CUNN8L4\\Li et al. - 2022 - Research on Lightweight Pedestrian Detection Model.pdf:application/pdf},
}

@inproceedings{kruthiventiLowlightPedestrianDetection2017,
	title = {Low-light pedestrian detection from {RGB} images using multi-modal knowledge distillation},
	doi = {10.1109/ICIP.2017.8297075},
	abstract = {While deep learning based pedestrian detection systems have continued to scale new heights in recent times, the performance of such algorithms tends to degrade under challenging illumination conditions. This causes a bottleneck in ready portability of such systems to Advanced Driver Assistance Systems (ADAS), where consistent performance across varying environmental lighting is desired. Inspired by the concept of dark knowledge, this paper proposes a novel guided deep network that distills knowledge from a multi-modal pedestrian detector. The proposed network learns to extract both RGB and thermal-like features from RGB images alone, thus compensating for the requirement of significantly costly automotive-grade thermal cameras. Compelling detection performance in severe lighting conditions is demonstrated on a publicly available night-time pedestrian dataset — KAIST. We achieve an effective miss-rate of 12\% lower than the recent state-of-the-art methods.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Kruthiventi, Srinivas S. S. and Sahay, Pratyush and Biswal, Rajesh},
	month = sep,
	year = {2017},
	note = {ISSN: 2381-8549},
	keywords = {Feature extraction, Training, Visualization, Convolutional Neural Networks, Data mining, Knowledge Distillation, Knowledge engineering, Lighting, Low-light pedestrian detection, Proposals},
	pages = {4207--4211},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\6NGKZRLZ\\8297075.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\VQ6T7RME\\Kruthiventi et al. - 2017 - Low-light pedestrian detection from RGB images usi.pdf:application/pdf},
}

@inproceedings{jeghamPedestrianDetectionPoor2017,
	title = {Pedestrian detection in poor weather conditions using moving camera},
	doi = {10.1109/AICCSA.2017.35},
	abstract = {Many challenges are present in the pedestrian detection field which makes it a trending topic. Detecting pedestrian is an extremely difficult task under bad weather conditions. In order to improve and facilitate the detection task, it is required to use infra-red images. For the advanced driver-assistance systems (ADAS), more specifically those of the pedestrian detection, the camera is mounted on a moving vehicle resulting egomotion in the background. Thus another challenging problem is added. It is then required to compensate the background egomotion to obtain a background static scene. In this paper, we introduce an advanced approach for the pedestrian detection under poor weather conditions using a moving camera. First, using the interest point detector Speeded Up Robust Features (SURF), ego-motions in the background are adjusted. After that, the foreground is detected by subtracting frames. Then, a segmentation step is required to divide the images into multiple moving objects. Finally, a recognition process is applied in order to classify the moving objects into both categories: pedestrian and undefined patterns. The proposed approach was evaluated on the CVC14 dataset. Experimental results illustrate the good performance of the approach.},
	booktitle = {2017 {IEEE}/{ACS} 14th {International} {Conference} on {Computer} {Systems} and {Applications} ({AICCSA})},
	author = {Jegham, Imen and Ben Khalifa, Anouar},
	month = oct,
	year = {2017},
	note = {ISSN: 2161-5330},
	keywords = {Image segmentation, Detectors, Pedestrian detection, Cameras, Egomotion compensation, Fear infra-red, Finite impulse response filters, Meteorology, Object recognition, Poor weather conditions, Roads},
	pages = {358--362},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\8TCZVCR7\\8308308.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\SG7VYGCC\\Jegham and Ben Khalifa - 2017 - Pedestrian detection in poor weather conditions us.pdf:application/pdf},
}

@misc{PeopleDetectionSystem,
	title = {People {Detection} {System} {Using} {YOLOv3} {Algorithm} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org.oca.rizal.library.remotexs.co/document/9204925},
	urldate = {2023-03-13},
	file = {People Detection System Using YOLOv3 Algorithm | IEEE Conference Publication | IEEE Xplore:H\:\\Zotero\\storage\\WWXHKNYD\\9204925.html:text/html},
}

@inproceedings{hassanPeopleDetectionSystem2020,
	title = {People {Detection} {System} {Using} {YOLOv3} {Algorithm}},
	doi = {10.1109/ICCSCE50387.2020.9204925},
	abstract = {In crowd security systems, precise real-time detection of people in images or videos can be very challenging especially in complex and dense crowds whereby some individuals could possibly be partly or entirely occluded for varying lengths of time. Thus, this paper presents a large Convolutional Neural Network (CNN) that is trained using a single step model, You Only Look Once version 3 (YOLOv3) on Google Colaboratory to process the images within a database and to accurately locate people within the images. YOLOv3 splits the image up into regions and predicts bounding boxes and predicts the probabilities for each region. These bounding boxes are weighted by the projected probabilities and finally, the model is able to make its detection based on the final weights. This model will be using a customised dataset from Google's Open Images with 500 high resolution images. Once trained, the neural network able to successfully generate the test data and achieve a mean average precision (mAP) of 78.3\% and a final average loss of 0.6 on top of confidently detecting the people within the images.},
	booktitle = {2020 10th {IEEE} {International} {Conference} on {Control} {System}, {Computing} and {Engineering} ({ICCSCE})},
	author = {Hassan, Nurul Iman and Tahir, Nooritawati Md. and Zaman, Fadhlan Hafizhelmi Kamaru and Hashim, Habibah},
	month = aug,
	year = {2020},
	keywords = {Training, convolutional neural network, Machine learning, Object detection, Conferences, deep learning, Google, Neural networks, Security, Surveillance, YOLOv3},
	pages = {131--136},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\IS5YY3VH\\9204925.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\66JGT7Q7\\Hassan et al. - 2020 - People Detection System Using YOLOv3 Algorithm.pdf:application/pdf},
}

@article{xuEfficientPedestrianDetection2022,
	title = {An {Efficient} {Pedestrian} {Detection} for {Realtime} {Surveillance} {Systems} {Based} on {Modified} {YOLOv3}},
	volume = {6},
	issn = {2469-7281},
	doi = {10.1109/JRFID.2022.3212907},
	abstract = {Pedestrian detection is an important branch of object detection due to its various applications. It plays a vital role in many fields such as intelligent surveillance systems. The recognition, identification and tracking modules of surveillance are based on efficient and accurate pedestrian detection. Our paper proposes an efficient model to solve real-time pedestrian detection with high accuracy based on modified ShuffleNet and YOLOv3 models. We provide a method to pick the dimensions and number of anchor boxes for predicting bounding boxes accurately. Then we use two improved shuffle units to lightweight the backbone of YOLOv3, which reduces the 67.5\% floating point operations per second (FLOPs) and 65.1\% parameters. We validate our model on CrowdHuman detection data set and get 62.7 mAP for face and 62.0 mAP person with 0.748 average IOU. Our network processes images in real-time at 186.1 frames per second for network and 12.5 frames per second for the entire model on CrowdHuman.},
	journal = {IEEE Journal of Radio Frequency Identification},
	author = {Xu, Ming and Wang, Zhen and Liu, Xingmao and Ma, Longhua and Shehzad, Ahsan},
	year = {2022},
	note = {Conference Name: IEEE Journal of Radio Frequency Identification},
	keywords = {Convolutional neural networks, Computational modeling, Object detection, Optimization, Real-time systems, YOLOv3, Computer vision, Convolutional neural network, K-means pedestrian detection, Radiofrequency identification, shuffle unit video surveillance},
	pages = {972--976},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\K4MVEIS4\\9913649.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\3L77KMMQ\\Xu et al. - 2022 - An Efficient Pedestrian Detection for Realtime Sur.pdf:application/pdf},
}

@inproceedings{rakhsithFaceMaskSocial2021,
	title = {Face {Mask} and {Social} {Distancing} {Detection} for {Surveillance} {Systems}},
	doi = {10.1109/ICOEI51242.2021.9452973},
	abstract = {There is a huge panic among the people in recent times due to the spread of communicable diseases. People are in close vicinity to one another when in closed spaces like shops, restaurants, classrooms, etc. There is also a cause for worry in workplaces regarding the safety of the workplace. This paper discusses about two models which can be used to detect the distance between people to ensure social distancing and to detect if people are wearing a mask which can be implemented to follow safety measures. To implement these models deep learning techniques are used. For the social distancing model object detection is done to detect humans and this is done through the YOLOv3. For the mask detection model, the MobileNetV2 is the algorithm which is used for classification. This is used to detect if the people are wearing a mask. These two models can be used for the purpose of prevention against widely spreading diseases. For example, if the people of an organization have to request their customers to stay 6 feet apart or wear a mask in cases where the customers are not following the standard safety protocols, the people of the organization should go directly up to them and request for it. This increases the contact between people and at the same time increases the risk factor for the people working in that organization. When these models are implemented, it reduces unnecessary human contact while also ensuring to alert the customers if they break these protocols.},
	booktitle = {2021 5th {International} {Conference} on {Trends} in {Electronics} and {Informatics} ({ICOEI})},
	author = {Rakhsith, L.A. and Karthik, B.E. and D, Arun Nithish and V, Kishore Kumar and Anusha, K.S.},
	month = jun,
	year = {2021},
	keywords = {Object detection, Surveillance, Employment, Human factors, Mask Detection, MobileNetV2, OpenCV, Organizations, Protocols, Social Distancing, Standards organizations, YOLOV3},
	pages = {1056--1065},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\UZFE2UFH\\9452973.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\VLLWQ6SX\\Rakhsith et al. - 2021 - Face Mask and Social Distancing Detection for Surv.pdf:application/pdf},
}

@inproceedings{kolpeIdentificationFaceMask2022,
	title = {Identification of {Face} {Mask} and {Social} {Distancing} using {YOLO} {Algorithm} based on {Machine} {Learning} {Approach}},
	doi = {10.1109/ICICCS53718.2022.9788241},
	abstract = {An outbreak of the coronavirus disease due to occur in 2020 has already had a significant impact on the human race. Wearing a "Face Mask" and maintaining "Social Distancing" are the only means to protect ourselves from this pandemic. Several service providers, such as airlines, hotels, hospitals, and train stations, demand their customers to access the service only if the Mask is worn correctly and social distance is maintained. Manually checking to see if the rule of mask wearing and social distance is being observed is impossible due to the significant human resource consumption. As a one-stage detector, the COVID-19 Face Mask and Social Distancing Detector System uses an artificial neural network to combine high-level semantic information with various feature maps and a machine learning module to identify face masks and social distances at the same time. It will also be able to detect persons without masks and the violence of social separation by using existing IP cameras, CCTV cameras, and computer vision. This technology eliminates the need for a manual surveillance system by providing instruments for safety and security. The technology can be used in any type of infrastructure, including hospitals, government offices, schools, and construction sites. Therefore, the face mask and social distance detector system developed, could aid to secure the protection and security of ourselves and our loved ones.},
	booktitle = {2022 6th {International} {Conference} on {Intelligent} {Computing} and {Control} {Systems} ({ICICCS})},
	author = {Kolpe, Rupali and Ghogare, Shubham and Jawale, M.A. and William, P. and Pawar, A.B.},
	month = may,
	year = {2022},
	note = {ISSN: 2768-5330},
	keywords = {Deep learning, Detectors, Machine learning, Surveillance, Human factors, OpenCV, COVID-19, Face Mask Detection, Fully Convolutional Network, Hospitals, ResNet50, Social Distance Detection, Social factors, Yolov3},
	pages = {1399--1403},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\IZQSNVYC\\9788241.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\KHLK4M6Y\\Kolpe et al. - 2022 - Identification of Face Mask and Social Distancing .pdf:application/pdf},
}

@inproceedings{mhadgutMaskedFaceDetection2021,
	title = {Masked {Face} {Detection} and {Recognition} {System} in {Real} {Time} using {YOLOv3} to combat {COVID}-19},
	doi = {10.1109/ICCCNT51525.2021.9579525},
	abstract = {A new challenge in the facial recognition technology is observed during the COVID-19 pandemic which has created a need for developing alternatives in face recognition algorithms that exist today. During this pandemic, masked faces have made face recognition applications used for security surveillance and attendance systems less effective. A comparative study was conducted on different YOLO models like YOLOv3, Tiny-YOLOv3, Tiny-YOLOv4 to judge their performances for the face detection module. From the study, it is concluded that the YOLOv3 model outperformed the other algorithms. Additionally, the face images captured from cameras were encoded and compared to determine the best face images for the face recognition module. It was identified that YOLOv3 along with face encodings from IP camera images accomplished an overall testing accuracy of 95.83\% on masked and unmasked faces. The system introduced a confidence level to further reduce the error while registering the identity of the person.},
	booktitle = {2021 12th {International} {Conference} on {Computing} {Communication} and {Networking} {Technologies} ({ICCCNT})},
	author = {Mhadgut, Sanika},
	month = jul,
	year = {2021},
	keywords = {Real-time systems, Cameras, Surveillance, YOLOv3, COVID-19, Face recognition, Image coding, Masked Face Detection, Masked Face Recognition, Pandemics},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\CM48VKYK\\9579525.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\WHW2GC2M\\Mhadgut - 2021 - Masked Face Detection and Recognition System in Re.pdf:application/pdf},
}

@inproceedings{sevillaMaskVisionMachineVisionBased2021,
	title = {Mask-{Vision}: {A} {Machine} {Vision}-{Based} {Inference} {System} of {Face} {Mask} {Detection} for {Monitoring} {Health} {Protocol} {Safety}},
	shorttitle = {Mask-{Vision}},
	doi = {10.1109/IICAIET51634.2021.9573664},
	abstract = {To avoid adversely affecting community health and the global economy, effective ways to limit the COVID-19 pandemic require constant attention. In the absence of efficient antivirals and insufficient medical resources, WHO recommends several methods to minimize infection rates and prevent depletion of scarce healthcare resources. One of the non-pharmaceutical treatments that can be used to decrease the primary source of SARS-CoV2 droplets expelled by an infected individual is to wear a mask. Irrespective of disagreements about medical resources and mask types, all governments enforce the wearing of masks that cover the nose and mouth by the general population. In the next years, the suggested mask detection models might be a valuable tool for ensuring that safety measures are followed correctly. The YOLOv3 model, a deep transfer learning object identification state-of-the-art approach, is used to create a mask detection model in this research article. The suggested model's exceptional performance makes it ideal for video surveillance equipment. The suggested approach focuses on creating an enhanced dataset from a 300-image dataset utilizing data augmentation techniques such as image filtering. The Data augmentation-based mask detection model's mean average precision was found to be 89.8\% during training and 100\% during overall testing, with detection per frame accuracy ranging from 40.03\% to 65.03\%.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Artificial} {Intelligence} in {Engineering} and {Technology} ({IICAIET})},
	author = {Sevilla, Rovenson V. and Alon, Alvin Sarraga and Melegrito, Mark P. and Reyes, Ryan Carreon and Bastes, Bobby M. and Cimagala, Roselle P.},
	month = sep,
	year = {2021},
	keywords = {Training, Tools, Video surveillance, deep learning, COVID-19, Biological system modeling, covid-19, face mask detection, object detection, Safety, Transfer learning, yolov3},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\6DXSE6KW\\9573664.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\5CUQAVZB\\Sevilla et al. - 2021 - Mask-Vision A Machine Vision-Based Inference Syst.pdf:application/pdf},
}

@inproceedings{islamComputationallyEffectivePedestrian2021,
	title = {A {Computationally} {Effective} {Pedestrian} {Detection} using {Constrained} {Fusion} with {Body} {Parts} for {Autonomous} {Driving}},
	doi = {10.1109/IRC52146.2021.00024},
	abstract = {This paper addresses the problem of detecting pedestrians using an enhanced object detection method. In particular, the paper considers the occluded pedestrian detection problem in autonomous driving scenarios where the balance of performance between accuracy and speed is crucial. Existing works focus on learning representations of unique persons independent of body parts semantics. To achieve a real-time performance along with robust detection, we introduce a body parts based pedestrian detection architecture where body parts are fused through a computationally effective constraint optimization technique. We demonstrate that our method significantly improves detection accuracy while adding negligible runtime overhead. We evaluate our method using a real-world dataset. Experimental results show that the proposed method outperforms existing pedestrian detection methods.},
	booktitle = {2021 {Fifth} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Islam, Muhammad Mobaidul and Newaz, Abdullah Al Redwan and Tian, Renran and Homaifar, Abdollah and Karimoddini, Ali},
	month = nov,
	year = {2021},
	keywords = {Object detection, Pedestrian Detection, Real-time systems, Conferences, Autonomous car, Autonomous Driving, Body parts, Computer architecture, Constraint optimization, Deap Learning, Fusion, Machine Learning, Runtime, Semantics},
	pages = {106--110},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\M3Z4EU37\\9699953.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\LNWBEY7Q\\Islam et al. - 2021 - A Computationally Effective Pedestrian Detection u.pdf:application/pdf},
}

@inproceedings{zhangPedestrianDetectionAlgorithm2021,
	title = {Pedestrian detection algorithm based on improved {Yolo} v3},
	doi = {10.1109/ICPICS52425.2021.9524267},
	abstract = {Pedestrian detection has always been a research hotspot and difficulty in the field of video analysis, and it has a wide range of applications in fields such as unmanned driving, road monitoring, and smart cities. Aiming at this problem, a pedestrian detection method based on the improved YOLOv3 algorithm is proposed. The software system is implemented and verified based on YOLO v3. Experimental results show that in pedestrian detection data sets such as the INRIA pedestrian data set, the accuracy of the algorithm is improved by 6.3\% compared with the original algorithm. The target detection technology can meet the real-time performance and test requirements in terms of pedestrian accuracy. Finally, the future development and further research directions of pedestrian detection technology are discussed.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Power}, {Intelligent} {Computing} and {Systems} ({ICPICS})},
	author = {Zhang, Jiaqi and Chen, Xunlei and Li, Yingling and Chen, Tianxiang and Mou, Liqiang},
	month = jul,
	year = {2021},
	keywords = {Object detection, Pedestrian detection, Roads, feature detection, Image recognition, Smart cities, Software algorithms, Software systems, Target recognition, yolo v3},
	pages = {180--183},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\675UBZ86\\9524267.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\F7LEWDLF\\Zhang et al. - 2021 - Pedestrian detection algorithm based on improved Y.pdf:application/pdf},
}

@inproceedings{ahmedEnhancedVulnerablePedestrian2019,
	title = {Enhanced {Vulnerable} {Pedestrian} {Detection} using {Deep} {Learning}},
	doi = {10.1109/ICCSP.2019.8697978},
	abstract = {Road forms an integral part of quotidian commute and yet remains equally unsafe for commuters especially for pedestrians. Among the various categories that exists for object detection, detecting pedestrians remain a daunting task owing to a large changeability in the background as well as severe overlapping or occlusion. Also achieving high accuracy and real time speed pose a serious challenge since pedestrian detection should be both accurate as well as fast enough to be deployed in real time. This paper addresses these requirements through the usage of depth wise separable convolution and Single Shot Detector framework employing different activation maps using OpenCV to achieve a reliable, robust and competent deep learning based pedestrian detection for real time operations.},
	booktitle = {2019 {International} {Conference} on {Communication} and {Signal} {Processing} ({ICCSP})},
	author = {Ahmed, Zahid and Iniyavan, R. and P., Madhan Mohan},
	month = apr,
	year = {2019},
	keywords = {Deep learning, Deep Learning, Detectors, Object detection, Convolution, Pedestrian Detection, Real-time systems, Neural networks, MobileNets, Network architecture, Single Shot Detectors},
	pages = {0971--0974},
	file = {IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\P4MTD57W\\Ahmed et al. - 2019 - Enhanced Vulnerable Pedestrian Detection using Dee.pdf:application/pdf},
}

@misc{bochkovskiyYOLOv4OptimalSpeed2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	doi = {10.48550/arXiv.2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv:2004.10934 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\TVBDQMHR\\Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\SWA2INN6\\2004.html:text/html},
}

@misc{YOLOV5Model2022,
	title = {{YOLO} v5 model architecture [{Explained}]},
	url = {https://iq.opengenus.org/yolov5/},
	abstract = {Since 2015 the Ultralytics team has been working on improving this model and many versions since then have been released. In this article we will take a look at the fifth version of this algorithm YOLOv5.},
	language = {en},
	urldate = {2023-03-14},
	journal = {OpenGenus IQ: Computing Expertise \& Legacy},
	month = oct,
	year = {2022},
	note = {Retrieved March 14, 2023 from https://iq.opengenus.org/yolov5/},
	file = {Snapshot:H\:\\Zotero\\storage\\RZMSBAHP\\yolov5.html:text/html},
}

@misc{jocherUltralyticsYolov5V72022,
	title = {ultralytics/yolov5: v7.0 - {YOLOv5} {SOTA} {Realtime} {Instance} {Segmentation}},
	shorttitle = {ultralytics/yolov5},
	url = {https://zenodo.org/record/7347926},
	abstract = {{\textless}div align="center"{\textgreater} {\textless}a align="center" href="https://ultralytics.com/yolov5" target="\_blank"{\textgreater} {\textless}img width="850" src="https://github.com/ultralytics/assets/blob/master/yolov5/v70/splash.png"{\textgreater}{\textless}/a{\textgreater} {\textless}/div{\textgreater} {\textless}br{\textgreater} Our new YOLOv5 v7.0 instance segmentation models are the fastest and most accurate in the world, beating all current SOTA benchmarks. We've made them super simple to train, validate and deploy. See full details in our Release Notes and visit our YOLOv5 Segmentation Colab Notebook for quickstart tutorials. {\textless}div align="center"{\textgreater} {\textless}a align="center" href="https://ultralytics.com/yolov5" target="\_blank"{\textgreater} {\textless}img width="800" src="https://user-images.githubusercontent.com/26833433/203348073-9b85607b-03e2-48e1-a6ba-fe1c1c31749c.png"{\textgreater}{\textless}/a{\textgreater} {\textless}/div{\textgreater} {\textless}br{\textgreater} Our primary goal with this release is to introduce super simple YOLOv5 segmentation workflows just like our existing object detection models. The new v7.0 YOLOv5-seg models below are just a start, we will continue to improve these going forward together with our existing detection and classification models. We'd love your feedback and contributions on this effort! This release incorporates 280 PRs from 41 contributors since our last release in August 2022. Important Updates Segmentation Models ⭐ NEW: SOTA YOLOv5-seg COCO-pretrained segmentation models are now available for the first time (https://github.com/ultralytics/yolov5/pull/9052 by @glenn-jocher, @AyushExel and @Laughing-q) Paddle Paddle Export: Export any YOLOv5 model (cls, seg, det) to Paddle format with python export.py --include paddle (https://github.com/ultralytics/yolov5/pull/9459 by @glenn-jocher) YOLOv5 AutoCache: Use python train.py --cache ram will now scan available memory and compare against predicted dataset RAM usage. This reduces risk in caching and should help improve adoption of the dataset caching feature, which can significantly speed up training. (https://github.com/ultralytics/yolov5/pull/10027 by @glenn-jocher) Comet Logging and Visualization Integration: Free forever, Comet lets you save YOLOv5 models, resume training, and interactively visualise and debug predictions. (https://github.com/ultralytics/yolov5/pull/9232 by @DN6) New Segmentation Checkpoints We trained YOLOv5 segmentations models on COCO for 300 epochs at image size 640 using A100 GPUs. We exported all models to ONNX FP32 for CPU speed tests and to TensorRT FP16 for GPU speed tests. We ran all speed tests on Google Colab Pro notebooks for easy reproducibility. Model size{\textless}br{\textgreater}{\textless}sup{\textgreater}(pixels) mAP{\textless}sup{\textgreater}box{\textless}br{\textgreater}50-95 mAP{\textless}sup{\textgreater}mask{\textless}br{\textgreater}50-95 Train time{\textless}br{\textgreater}{\textless}sup{\textgreater}300 epochs{\textless}br{\textgreater}A100 (hours) Speed{\textless}br{\textgreater}{\textless}sup{\textgreater}ONNX CPU{\textless}br{\textgreater}(ms) Speed{\textless}br{\textgreater}{\textless}sup{\textgreater}TRT A100{\textless}br{\textgreater}(ms) params{\textless}br{\textgreater}{\textless}sup{\textgreater}(M) FLOPs{\textless}br{\textgreater}{\textless}sup{\textgreater}@640 (B) YOLOv5n-seg 640 27.6 23.4 80:17 62.7 1.2 2.0 7.1 YOLOv5s-seg 640 37.6 31.7 88:16 173.3 1.4 7.6 26.4 YOLOv5m-seg 640 45.0 37.1 108:36 427.0 2.2 22.0 70.8 YOLOv5l-seg 640 49.0 39.9 66:43 (2x) 857.4 2.9 47.9 147.7 YOLOv5x-seg 640 50.7 41.4 62:56 (3x) 1579.2 4.5 88.8 265.7 All checkpoints are trained to 300 epochs with SGD optimizer with lr0=0.01 and weight\_decay=5e-5 at image size 640 and all default settings.{\textless}br{\textgreater}Runs logged to https://wandb.ai/glenn-jocher/YOLOv5\_v70\_official Accuracy values are for single-model single-scale on COCO dataset.{\textless}br{\textgreater}Reproduce by python segment/val.py --data coco.yaml --weights yolov5s-seg.pt Speed averaged over 100 inference images using a Colab Pro A100 High-RAM instance. Values indicate inference speed only (NMS adds about 1ms per image). {\textless}br{\textgreater}Reproduce by python segment/val.py --data coco.yaml --weights yolov5s-seg.pt --batch 1 Export to ONNX at FP32 and TensorRT at FP16 done with export.py. {\textless}br{\textgreater}Reproduce by python export.py --weights yolov5s-seg.pt --include engine --device 0 --half New Segmentation Usage Examples Train YOLOv5 segmentation training supports auto-download COCO128-seg segmentation dataset with --data coco128-seg.yaml argument and manual download of COCO-segments dataset with bash data/scripts/get\_coco.sh --train --val --segments and then python train.py --data coco.yaml. \# Single-GPU python segment/train.py --model yolov5s-seg.pt --data coco128-seg.yaml --epochs 5 --img 640 \# Multi-GPU DDP python -m torch.distributed.run --nproc\_per\_node 4 --master\_port 1 segment/train.py --model yolov5s-seg.pt --data coco128-seg.yaml --epochs 5 --img 640 --device 0,1,2,3 Val Validate YOLOv5m-seg accuracy on ImageNet-1k dataset: bash data/scripts/get\_coco.sh --val --segments \# download COCO val segments split (780MB, 5000 images) python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640 \# validate Predict Use pretrained YOLOv5m-seg to predict bus.jpg: python segment/predict.py --weights yolov5m-seg.pt --data data/images/bus.jpg model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m-seg.pt') \# load from PyTorch Hub (WARNING: inference not yet supported) Export Export YOLOv5s-seg model to ONNX and TensorRT: python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0 Changelog Changes between previous release and this release: https://github.com/ultralytics/yolov5/compare/v6.2...v7.0 Changes since this release: https://github.com/ultralytics/yolov5/compare/v7.0...HEAD {\textless}details{\textgreater} {\textless}summary{\textgreater}🛠️ New Features and Bug Fixes (280){\textless}/summary{\textgreater} * Improve classification comments by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/8997 * Update `attempt\_download(release='v6.2')` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/8998 * Update README\_cn.md by @KieraMengru0907 in https://github.com/ultralytics/yolov5/pull/9001 * Update dataset `names` from array to dictionary by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9000 * [segment]: Allow inference on dirs and videos by @AyushExel in https://github.com/ultralytics/yolov5/pull/9003 * DockerHub tag update Usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9005 * Add weight `decay` to argparser by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9006 * Add glob quotes to detect.py usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9007 * Fix TorchScript JSON string key bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9015 * EMA FP32 assert classification bug fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9016 * Faster pre-processing for gray image input by @cher-liang in https://github.com/ultralytics/yolov5/pull/9009 * Improved `Profile()` inference timing by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9024 * `torch.empty()` for speed improvements by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9025 * Remove unused `time\_sync` import by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9026 * Add PyTorch Hub classification CI checks by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9027 * Attach transforms to model by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9028 * Default --data `imagenette160` training (fastest) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9033 * VOC `names` dictionary fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9034 * Update train.py `import val as validate` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9037 * AutoBatch protect from negative batch sizes by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9048 * Temporarily remove `macos-latest` from CI by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9049 * Add `--save-hybrid` mAP warning by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9050 * Refactor for simplification by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9054 * Refactor for simplification 2 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9055 * zero-mAP fix return `.detach()` to EMA by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9056 * zero-mAP fix 3 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9058 * Daemon `plot\_labels()` for faster start by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9057 * TensorBoard fix in tutorial.ipynb by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9064 * zero-mAP fix remove `torch.empty()` forward pass in `.train()` mode by @0zppd in https://github.com/ultralytics/yolov5/pull/9068 * Rename 'labels' to 'instances' by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9066 * Threaded TensorBoard graph logging by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9070 * De-thread TensorBoard graph logging by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9071 * Two dimensional `size=(h,w)` AutoShape support by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9072 * Remove unused Timeout import by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9073 * Improved Usage example docstrings by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9075 * Install `torch` latest stable by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9092 * New `@try\_export` decorator by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9096 * Add optional `transforms` argument to LoadStreams() by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9105 * Streaming Classification support by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9106 * Fix numpy to torch cls streaming bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9112 * Infer Loggers project name by @AyushExel in https://github.com/ultralytics/yolov5/pull/9117 * Add CSV logging to GenericLogger by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9128 * New TryExcept decorator by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9154 * Fixed segment offsets by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9155 * New YOLOv5 v6.2 splash images by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9142 * Rename onnx\_dynamic -{\textgreater} dynamic by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9168 * Inline `\_make\_grid()` meshgrid by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9170 * Comment EMA assert by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9173 * Fix confidence threshold for ClearML debug images by @HighMans in https://github.com/ultralytics/yolov5/pull/9174 * Update Dockerfile-cpu by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9184 * Update Dockerfile-cpu to libpython3-dev by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9185 * Update Dockerfile-arm64 to libpython3-dev by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9187 * Fix AutoAnchor MPS bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9188 * Skip AMP check on MPS by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9189 * ClearML's set\_report\_period's time is defined in minutes not seconds. by @HighMans in https://github.com/ultralytics/yolov5/pull/9186 * Add `check\_git\_status(..., branch='master')` argument by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9199 * `check\_font()` on notebook init by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9200 * Comment `protobuf` in requirements.txt by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9207 * `check\_font()` fstring update by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9208 * AutoBatch protect from extreme batch sizes by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9209 * Default AutoBatch 0.8 fraction by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9212 * Delete rebase.yml by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9202 * Duplicate segment verification fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9225 * New `LetterBox(size)` `CenterCrop(size)`, `ToTensor()` transforms (\#9213) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9213 * Add ClassificationModel TF export assert by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9226 * Remove usage of `pathlib.Path.unlink(missing\_ok=...)` by @ymerkli in https://github.com/ultralytics/yolov5/pull/9227 * Add support for `*.pfm` images by @spacewalk01 in https://github.com/ultralytics/yolov5/pull/9230 * Python check warning emoji by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9238 * Add `url\_getsize()` function by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9247 * Update dataloaders.py by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9250 * Refactor Loggers : Move code outside train.py by @AyushExel in https://github.com/ultralytics/yolov5/pull/9241 * Update general.py by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9252 * Add LoadImages.\_cv2\_rotate() by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9249 * Move `cudnn.benchmarks(True)` to LoadStreams by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9258 * `cudnn.benchmark = True` on Seed 0 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9259 * Update `TryExcept(msg='...')`` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9261 * Make sure best.pt model file is preserved ClearML by @thepycoder in https://github.com/ultralytics/yolov5/pull/9265 * DetectMultiBackend improvements by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9269 * Update DetectMultiBackend for tuple outputs by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9274 * Update DetectMultiBackend for tuple outputs 2 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9275 * Update benchmarks CI with `--hard-fail` min metric floor by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9276 * Add new `--vid-stride` inference parameter for videos by @VELCpro in https://github.com/ultralytics/yolov5/pull/9256 * [pre-commit.ci] pre-commit suggestions by @pre-commit-ci in https://github.com/ultralytics/yolov5/pull/9295 * Replace deprecated `np.int` with `int` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9307 * Comet Logging and Visualization Integration by @DN6 in https://github.com/ultralytics/yolov5/pull/9232 * Comet changes by @DN6 in https://github.com/ultralytics/yolov5/pull/9328 * Train.py line 486 typo fix by @robinned in https://github.com/ultralytics/yolov5/pull/9330 * Add dilated conv support by @YellowAndGreen in https://github.com/ultralytics/yolov5/pull/9347 * Update `check\_requirements()` single install by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9353 * Update `check\_requirements(args, cmds='')` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9355 * Update `check\_requirements()` multiple string by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9356 * Add PaddlePaddle export and inference by @kisaragychihaya in https://github.com/ultralytics/yolov5/pull/9240 * PaddlePaddle Usage examples by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9358 * labels.jpg names fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9361 * Exclude `ipython` from hubconf.py `check\_requirements()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9362 * `torch.jit.trace()` fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9363 * AMP Check fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9367 * Remove duplicate line in setup.cfg by @zldrobit in https://github.com/ultralytics/yolov5/pull/9380 * Remove `.train()` mode exports by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9429 * Continue on Docker arm64 failure by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9430 * Continue on Docker failure (all backends) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9432 * Continue on Docker fail (all backends) fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9433 * YOLOv5 segmentation model support by @AyushExel in https://github.com/ultralytics/yolov5/pull/9052 * Fix val.py zero-TP bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9431 * New model.yaml `activation:` field by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9371 * Fix tick labels for background FN/FP by @hotohoto in https://github.com/ultralytics/yolov5/pull/9414 * Fix TensorRT exports to ONNX opset 12 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9441 * AutoShape explicit arguments fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9443 * Update Detections() instance printing by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9445 * AutoUpdate TensorFlow in export.py by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9447 * AutoBatch `cudnn.benchmark=True` fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9448 * Do not move downloaded zips by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9455 * Update general.py by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9454 * `Detect()` and `Segment()` fixes for CoreML and Paddle by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9458 * Add Paddle exports to benchmarks by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9459 * Add `macos-latest` runner for CoreML benchmarks by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9453 * Fix cutout bug by @Oswells in https://github.com/ultralytics/yolov5/pull/9452 * Optimize imports by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9464 * TensorRT SegmentationModel fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9465 * `Conv()` dilation argument fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9466 * Update ClassificationModel default training `imgsz=224` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9469 * Standardize warnings with `WARNING ⚠️ ...` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9467 * TensorFlow macOS AutoUpdate by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9471 * `segment/predict --save-txt` fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9478 * TensorFlow SegmentationModel support by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9472 * AutoBatch report include reserved+allocated by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9491 * Update Detect() grid init `for` loop by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9494 * Accelerate video inference by @mucunwuxian in https://github.com/ultralytics/yolov5/pull/9487 * Comet Image Logging Fix by @DN6 in https://github.com/ultralytics/yolov5/pull/9498 * Fix visualization title bug by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9500 * Add paddle tips by @Zengyf-CVer in https://github.com/ultralytics/yolov5/pull/9502 * Segmentation `polygons2masks\_overlap()` in `np.int32` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9493 * Fix `random\_perspective` param bug in segment by @FeiGeChuanShu in https://github.com/ultralytics/yolov5/pull/9512 * Remove `check\_requirements('flatbuffers==1.12')` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9514 * Fix TF Lite exports by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9517 * TFLite fix 2 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9518 * FROM nvcr.io/nvidia/pytorch:22.08-py3 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9520 * Remove scikit-learn constraint on coremltools 6.0 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9530 * Update scikit-learn constraint per coremltools 6.0 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9531 * Update `coremltools{\textgreater}=6.0` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9532 * Update albumentations by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9503 * import re by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9535 * TF.js fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9536 * Refactor dataset batch-size by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9551 * Add `--source screen` for screenshot inference by @zombob in https://github.com/ultralytics/yolov5/pull/9542 * Update `is\_url()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9566 * Detect.py supports running against a Triton container by @gaziqbal in https://github.com/ultralytics/yolov5/pull/9228 * New `scale\_segments()` function by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9570 * generator seed fix for DDP mAP drop by @Forever518 in https://github.com/ultralytics/yolov5/pull/9545 * Update default GitHub assets by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9573 * Update requirements.txt comment https://pytorch.org/get-started/locally/ by @davidamacey in https://github.com/ultralytics/yolov5/pull/9576 * Add segment line predictions by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9571 * TensorRT detect.py inference fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9581 * Update Comet links by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9587 * Add global YOLOv5\_DATASETS\_DIR by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9586 * Add Paperspace Gradient badges by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9588 * \#YOLOVISION22 announcement by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9590 * Bump actions/stale from 5 to 6 by @dependabot in https://github.com/ultralytics/yolov5/pull/9595 * \#YOLOVISION22 update by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9598 * Apple MPS -{\textgreater} CPU NMS fallback strategy by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9600 * Updated Segmentation and Classification usage by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9607 * Update export.py Usage examples by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9609 * Fix `is\_url('https://ultralytics.com')` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9610 * Add `results.save(save\_dir='path', exist\_ok=False)` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9617 * NMS MPS device wrapper by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9620 * Add SegmentationModel unsupported warning by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9632 * Disabled upload\_dataset flag temporarily due to an artifact related bug by @soumik12345 in https://github.com/ultralytics/yolov5/pull/9652 * Add NVIDIA Jetson Nano Deployment tutorial by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9656 * Added cutout import from utils/augmentations.py to use Cutout Aug in … by @senhorinfinito in https://github.com/ultralytics/yolov5/pull/9668 * Simplify val.py benchmark mode with speed mode by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9674 * Allow list for Comet artifact class 'names' field by @KristenKehrer in https://github.com/ultralytics/yolov5/pull/9654 * [pre-commit.ci] pre-commit suggestions by @pre-commit-ci in https://github.com/ultralytics/yolov5/pull/9685 * TensorRT `--dynamic` fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9691 * FROM nvcr.io/nvidia/pytorch:22.09-py3 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9711 * Error in utils/segment/general `masks2segments()` by @paulguerrie in https://github.com/ultralytics/yolov5/pull/9724 * Fix segment evolution keys by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9742 * Remove \#YOLOVISION22 notice by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9751 * Update Loggers by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9760 * update mask2segments and saving results by @vladoossss in https://github.com/ultralytics/yolov5/pull/9785 * HUB VOC fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9792 * Update hubconf.py local repo Usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9803 * Fix xView dataloaders import by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9807 * Argoverse HUB fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9809 * `smart\_optimizer()` revert to weight with decay by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9817 * Allow PyTorch Hub results to display in notebooks by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9825 * Logger Cleanup by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9828 * Remove ipython from `check\_requirements` exclude list by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9841 * Update HUBDatasetStats() usage examples by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9842 * Update ZipFile to context manager by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9843 * Update README.md by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9846 * Webcam show fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9847 * Fix OpenVINO Usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9874 * ClearML Dockerfile fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9876 * Windows Python 3.7 .isfile() fix by @SSTato in https://github.com/ultralytics/yolov5/pull/9879 * Add TFLite Metadata to TFLite and Edge TPU models by @paradigmn in https://github.com/ultralytics/yolov5/pull/9903 * Add `gnupg` to Dockerfile-cpu by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9932 * Add ClearML minimum version requirement by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9933 * Update Comet Integrations table text by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9937 * Update README.md by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9957 * Update README.md by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9958 * Update README.md by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9961 * Switch from suffix checks to archive checks by @kalenmike in https://github.com/ultralytics/yolov5/pull/9963 * FROM nvcr.io/nvidia/pytorch:22.10-py3 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9966 * Full-size proto code (optional) by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9980 * Update README.md by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9970 * Segmentation Tutorial by @paulguerrie in https://github.com/ultralytics/yolov5/pull/9521 * Fix `is\_colab()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9994 * Check online twice on AutoUpdate by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9999 * Add `min\_items` filter option by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9997 * Improved `check\_online()` robustness by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10000 * Fix `min\_items` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10001 * Update default `--epochs 100` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10024 * YOLOv5 AutoCache Update by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10027 * IoU `eps` adjustment by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10051 * Update get\_coco.sh by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10057 * [pre-commit.ci] pre-commit suggestions by @pre-commit-ci in https://github.com/ultralytics/yolov5/pull/10068 * Use MNIST160 by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10069 * Update Dockerfile keep default torch installation by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10071 * Add `ultralytics` pip package by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10103 * AutoShape integer image-size fix by @janus-zheng in https://github.com/ultralytics/yolov5/pull/10090 * YouTube Usage example comments by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10106 * Mapped project and name to ClearML by @thepycoder in https://github.com/ultralytics/yolov5/pull/10100 * Update IoU functions by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10123 * Add Ultralytics HUB to README by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10070 * Fix benchmark.py usage comment by @rusamentiaga in https://github.com/ultralytics/yolov5/pull/10131 * Update HUB banner image by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10134 * Copy-Paste zero value fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10152 * Add Copy-Paste to `mosaic9()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10165 * Add `join\_threads()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10086 * Fix dataloader filepath modification to perform replace only once and not for all occurences of string by @adumrewal in https://github.com/ultralytics/yolov5/pull/10163 * fix: prevent logging config clobbering by @rkechols in https://github.com/ultralytics/yolov5/pull/10133 * Filter PyTorch 1.13 UserWarnings by @triple-Mu in https://github.com/ultralytics/yolov5/pull/10166 * Revert "fix: prevent logging config clobbering" by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10177 * Apply make\_divisible for ONNX models in Autoshape by @janus-zheng in https://github.com/ultralytics/yolov5/pull/10172 * data.yaml `names.keys()` integer assert by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10190 * fix: try 2 - prevent logging config clobbering by @rkechols in https://github.com/ultralytics/yolov5/pull/10192 * Segment prediction labels normalization fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10205 * Simplify dataloader tqdm descriptions by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10210 * New global `TQDM\_BAR\_FORMAT` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10211 * Feature/classification tutorial refactor by @paulguerrie in https://github.com/ultralytics/yolov5/pull/10039 * Remove Colab notebook High-Memory notices by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10212 * Revert `--save-txt` to default False by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10213 * Add `--source screen` Usage example by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10215 * Add `git` info to training checkpoints by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/9655 * Add git info to cls, seg checkpoints by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10217 * Update Comet preview image by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10220 * Scope gitpyhon import in `check\_git\_info()` by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10221 * Squeezenet reshape outputs fix by @glenn-jocher in https://github.com/ultralytics/yolov5/pull/10222 {\textless}/details{\textgreater}{\textless}details{\textgreater} {\textless}summary{\textgreater}😃 New Contributors (30){\textless}/summary{\textgreater} * @KieraMengru0907 made their first contribution in https://github.com/ultralytics/yolov5/pull/9001 * @cher-liang made their first contribution in https://github.com/ultralytics/yolov5/pull/9009 * @0zppd made their first contribution in https://github.com/ultralytics/yolov5/pull/9068 * @HighMans made their first contribution in https://github.com/ultralytics/yolov5/pull/9174 * @ymerkli made their first contribution in https://github.com/ultralytics/yolov5/pull/9227 * @spacewalk01 made their first contribution in https://github.com/ultralytics/yolov5/pull/9230 * @VELCpro made their first contribution in https://github.com/ultralytics/yolov5/pull/9256 * @DN6 made their first contribution in https://github.com/ultralytics/yolov5/pull/9232 * @robinned made their first contribution in https://github.com/ultralytics/yolov5/pull/9330 * @kisaragychihaya made their first contribution in https://github.com/ultralytics/yolov5/pull/9240 * @hotohoto made their first contribution in https://github.com/ultralytics/yolov5/pull/9414 * @Oswells made their first contribution in https://github.com/ultralytics/yolov5/pull/9452 * @mucunwuxian made their first contribution in https://github.com/ultralytics/yolov5/pull/9487 * @FeiGeChuanShu made their first contribution in https://github.com/ultralytics/yolov5/pull/9512 * @zombob made their first contribution in https://github.com/ultralytics/yolov5/pull/9542 * @gaziqbal made their first contribution in https://github.com/ultralytics/yolov5/pull/9228 * @Forever518 made their first contribution in https://github.com/ultralytics/yolov5/pull/9545 * @davidamacey made their first contribution in https://github.com/ultralytics/yolov5/pull/9576 * @soumik12345 made their first contribution in https://github.com/ultralytics/yolov5/pull/9652 * @senhorinfinito made their first contribution in https://github.com/ultralytics/yolov5/pull/9668 * @KristenKehrer made their first contribution in https://github.com/ultralytics/yolov5/pull/9654 * @paulguerrie made their first contribution in https://github.com/ultralytics/yolov5/pull/9724 * @vladoossss made their first contribution in https://github.com/ultralytics/yolov5/pull/9785 * @SSTato made their first contribution in https://github.com/ultralytics/yolov5/pull/9879 * @janus-zheng made their first contribution in https://github.com/ultralytics/yolov5/pull/10090 * @rusamentiaga made their first contribution in https://github.com/ultralytics/yolov5/pull/10131 * @adumrewal made their first contribution in https://github.com/ultralytics/yolov5/pull/10163 * @rkechols made their first contribution in https://github.com/ultralytics/yolov5/pull/10133 * @triple-Mu made their first contribution in https://github.com/ultralytics/yolov5/pull/10166 {\textless}/details{\textgreater}},
	urldate = {2023-03-14},
	publisher = {Zenodo},
	author = {Jocher, Glenn and Chaurasia, Ayush and Stoken, Alex and Borovec, Jirka and NanoCode012 and Kwon, Yonghye and Michael, Kalen and TaoXie and Fang, Jiacong and imyhxy and Lorna and Yifu), 曾逸夫(Zeng and Wong, Colin and V, Abhiram and Montes, Diego and Wang, Zhiqiang and Fati, Cristi and Nadar, Jebastin and Laughing and UnglvKitDe and Sonck, Victor and tkianai and yxNONG and Skalski, Piotr and Hogan, Adam and Nair, Dhruv and Strobel, Max and Jain, Mrinal},
	month = nov,
	year = {2022},
	doi = {10.5281/zenodo.7347926},
}

@misc{cholletXceptionDeepLearning2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv:1610.02357 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\F5NNLK5M\\Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\FYRQFYCU\\1610.html:text/html},
}

@misc{PapersCodeDepthwise,
	title = {Papers with {Code} - {Depthwise} {Separable} {Convolution} {Explained}},
	url = {https://paperswithcode.com/method/depthwise-separable-convolution},
	abstract = {While standard convolution performs the channelwise and spatial-wise computation in one step, Depthwise Separable Convolution  splits the computation into two steps: depthwise convolution applies a single convolutional filter per each input channel and pointwise convolution is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown to the right.

Credit: Depthwise Convolution Is All You Need for Learning Multiple Visual Domains},
	language = {en},
	urldate = {2023-03-15},
	file = {Snapshot:H\:\\Zotero\\storage\\XBCCA8VC\\depthwise-separable-convolution.html:text/html},
}

@misc{PhilippinesCovidLatest,
	title = {Philippines {Covid} {Latest}: {Mask} {Use} {Indoors} to {Be} {Made} {Voluntary} - {Bloomberg}},
	url = {https://www.bloomberg.com/news/articles/2022-10-25/philippines-to-make-voluntary-mask-use-indoors-voluntary?leadSource=uverify%20wall},
	urldate = {2023-03-20},
	file = {Philippines Covid Latest\: Mask Use Indoors to Be Made Voluntary - Bloomberg:H\:\\Zotero\\storage\\D9CWJHFJ\\philippines-to-make-voluntary-mask-use-indoors-voluntary.html:text/html},
}

@article{luOptimizingDepthwiseSeparable2022,
	title = {Optimizing {Depthwise} {Separable} {Convolution} {Operations} on {GPUs}},
	volume = {33},
	issn = {1045-9219},
	url = {https://doi.org/10.1109/TPDS.2021.3084813},
	doi = {10.1109/TPDS.2021.3084813},
	abstract = {The depthwise separable convolution is commonly seen in convolutional neural networks (CNNs), and is widely used to reduce the computation overhead of a standard multi-channel 2D convolution. Existing implementations of depthwise separable convolutions target accelerating model training with large batch sizes with a large number of samples to be processed at once. Such approaches are inadequate for small-batch-sized model training and the typical scenario of model inference where the model takes in a few samples at once. This article aims to bridge the gap of optimizing depthwise separable convolutions by targeting the GPU architecture. We achieve this by designing two novel algorithms to improve the column and row reuse of the convolution operation to reduce the number of memory operations performed on the width and the height dimensions of the 2D convolution. Our approach employs a dynamic tile size scheme to adaptively distribute the computational data across GPU threads to improve GPU utilization and to hide the memory access latency. We apply our approach on two GPU platforms: an NVIDIA RTX 2080Ti GPU and an embedded NVIDIA Jetson AGX Xavier GPU, and two data types: 32-bit floating point (FP32) and 8-bit integer (INT8). We compared our approach against cuDNN that is heavily tuned for the NVIDIA GPU architecture. Experimental results show that our approach delivers over {\textless}inline-formula{\textgreater}{\textless}tex-math notation="LaTeX"{\textgreater}\$2{\textbackslash}times\${\textless}/tex-math{\textgreater}{\textless}alternatives{\textgreater}{\textless}mml:math{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mn{\textgreater}2{\textless}/mml:mn{\textgreater}{\textless}mml:mo{\textgreater}\&\#x00D7;{\textless}/mml:mo{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}{\textless}inline-graphic xlink:href="lu-ieq1-3084813.gif"/{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} (up to {\textless}inline-formula{\textgreater}{\textless}tex-math notation="LaTeX"{\textgreater}\$3{\textbackslash}times\${\textless}/tex-math{\textgreater}{\textless}alternatives{\textgreater}{\textless}mml:math{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mn{\textgreater}3{\textless}/mml:mn{\textgreater}{\textless}mml:mo{\textgreater}\&\#x00D7;{\textless}/mml:mo{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}{\textless}inline-graphic xlink:href="lu-ieq2-3084813.gif"/{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater}) performance improvement over cuDNN. We show that, when using a moderate batch size, our approach averagely reduces the end-to-end training time of MobileNet and EfficientNet by 9.7 and 7.3 percent respectively, and reduces the end-to-end inference time of MobileNet and EfficientNet by 12.2 and 11.6 percent respectively.},
	number = {1},
	urldate = {2023-04-03},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Lu, Gangzhao and Zhang, Weizhe and Wang, Zheng},
	month = jan,
	year = {2022},
	pages = {70--87},
	file = {Accepted Version:H\:\\Zotero\\storage\\H2XACDWR\\Lu et al. - 2022 - Optimizing Depthwise Separable Convolution Operati.pdf:application/pdf},
}

@article{luOptimizingDepthwiseSeparable2022a,
	title = {Optimizing {Depthwise} {Separable} {Convolution} {Operations} on {GPUs}},
	volume = {33},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/9444208/},
	doi = {10.1109/TPDS.2021.3084813},
	abstract = {The depthwise separable convolution is commonly seen in convolutional neural networks (CNNs), and is widely used to reduce the computation overhead of a standard multi-channel 2D convolution. Existing implementations of depthwise separable convolutions target accelerating model training with large batch sizes with a large number of samples to be processed at once. Such approaches are inadequate for small-batch-sized model training and the typical scenario of model inference where the model takes in a few samples at once. This paper aims to bridge the gap of optimizing depthwise separable convolutions by targeting the GPU architecture. We achieve this by designing two novel algorithms to improve the column and row reuse of the convolution operation to reduce the number of memory operations performed on the width and the height dimensions of the 2D convolution. Our approach employs a dynamic tile size scheme to adaptively distribute the computational data across GPU threads to improve GPU utilization and to hide the memory access latency. We apply our approach on two GPU platforms: an NVIDIA RTX 2080Ti GPU and an embedded NVIDIA Jetson AGX Xavier GPU, and two data types: 32-bit ﬂoating point (FP32) and 8-bit integer (INT8). We compared our approach against cuDNN that is heavily tuned for the NVIDIA GPU architecture. Experimental results show that our approach delivers over 2× (up to 3×) performance improvement over cuDNN. We show that, when using a moderate batch size, our approach averagely reduces the end-to-end training time of MobileNetV2 and EfﬁcientNet-B0 by 9.7\% and 7.3\% respectively, and reduces the end-to-end inference time of MobileNet and EfﬁcentNet by 12.2\% and 13.5\% respectively.},
	language = {en},
	number = {1},
	urldate = {2023-04-03},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Lu, Gangzhao and Zhang, Weizhe and Wang, Zheng},
	month = jan,
	year = {2022},
	pages = {70--87},
	file = {Lu et al. - 2022 - Optimizing Depthwise Separable Convolution Operati.pdf:H\:\\Zotero\\storage\\R8CU7VNH\\Lu et al. - 2022 - Optimizing Depthwise Separable Convolution Operati.pdf:application/pdf},
}

@incollection{chakarDepthwiseSeparableConvolutions2020,
	address = {Cham},
	title = {Depthwise {Separable} {Convolutions} and {Variational} {Dropout} within the context of {YOLOv3}},
	volume = {12509},
	isbn = {978-3-030-64555-7 978-3-030-64556-4},
	url = {https://link.springer.com/10.1007/978-3-030-64556-4_9},
	abstract = {Deep learning algorithms have demonstrated remarkable performance in many sectors and have become one of the main foundations of modern computer-vision solutions. However, these algorithms often impose prohibitive levels of memory and computational overhead, especially in resource-constrained environments. In this study, we combine the state-of-the-art object-detection model YOLOv3 with depthwise separable convolutions and variational dropout in an attempt to bridge the gap between the superior accuracy of convolutional neural networks and the limited access to computational resources. We propose three lightweight variants of YOLOv3 by replacing the original network’s standard convolutions with depthwise separable convolutions at different strategic locations within the network, and we evaluate their impacts on YOLOv3’s size, speed, and accuracy. We also explore variational dropout: a technique that finds individual and unbounded dropout rates for each neural network weight. Experiments on the PASCAL VOC benchmark dataset show promising results where variational dropout combined with the most efficient YOLOv3 variant lead to an extremely sparse solution that reduces 95\% of the baseline network’s parameters at a relatively small drop of 3\% in accuracy.},
	language = {en},
	urldate = {2023-04-03},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Chakar, Joseph and Sobbahi, Rayan Al and Tekli, Joe},
	editor = {Bebis, George and Yin, Zhaozheng and Kim, Edward and Bender, Jan and Subr, Kartic and Kwon, Bum Chul and Zhao, Jian and Kalkofen, Denis and Baciu, George},
	year = {2020},
	doi = {10.1007/978-3-030-64556-4_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {107--120},
	file = {Chakar et al. - 2020 - Depthwise Separable Convolutions and Variational D.pdf:H\:\\Zotero\\storage\\XSV5L8I5\\Chakar et al. - 2020 - Depthwise Separable Convolutions and Variational D.pdf:application/pdf},
}

@misc{velezDepthwiseSeparableConvolutionsPytorch2022,
	title = {Depthwise-{Separable} convolutions in {Pytorch}},
	url = {https://faun.pub/depthwise-separable-convolutions-in-pytorch-fd41a97327d0},
	abstract = {Learn how to do depthwise-separable convolutions in Pytorch},
	language = {en},
	urldate = {2023-04-03},
	journal = {Medium},
	author = {Velez, Diego},
	month = jun,
	year = {2022},
}

@misc{DepthwiseSeparableConvolutions0611,
	title = {Depthwise {Separable} {Convolutions} in {PyTorch} :: {Päpper}'s {Machine} {Learning} {Blog} — {This} blog features state of the art applications in machine learning with a lot of {PyTorch} samples and deep learning code. {You} will learn about neural network optimization and potential insights for artificial intelligence for example in the medical domain.},
	shorttitle = {Depthwise {Separable} {Convolutions} in {PyTorch}},
	url = {https://www.paepper.com/blog/posts/depthwise-separable-convolutions-in-pytorch/},
	abstract = {In many neural network architectures like MobileNets, depthwise separable convolutions are used instead of regular convolutions. They have been shown to yield similar performance while being much more efficient in terms of using much less parameters and less floating point operations (FLOPs). Today, we will take a look at the difference of depthwise separable convolutions to standard convolutions and will analyze where the efficiency comes from.
Short recap: standard convolution In standard convolutions, we are analyzing an input map of height H and width W comprised of C channels.},
	language = {en},
	urldate = {2023-04-03},
	journal = {Depthwise Separable Convolutions in PyTorch},
	year = {0611},
	note = {Section: Machine Learning},
	file = {Snapshot:H\:\\Zotero\\storage\\RHLI2MR4\\depthwise-separable-convolutions-in-pytorch.html:text/html},
}

@article{liYOLOv3LiteLightweightCrack2019,
	title = {{YOLOv3}-{Lite}: {A} {Lightweight} {Crack} {Detection} {Network} for {Aircraft} {Structure} {Based} on {Depthwise} {Separable} {Convolutions}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {{YOLOv3}-{Lite}},
	url = {https://www.mdpi.com/2076-3417/9/18/3781},
	doi = {10.3390/app9183781},
	abstract = {Due to the high proportion of aircraft faults caused by cracks in aircraft structures, crack inspection in aircraft structures has long played an important role in the aviation industry. The existing approaches, however, are time-consuming or have poor accuracy, given the complex background of aircraft structure images. In order to solve these problems, we propose the YOLOv3-Lite method, which combines depthwise separable convolution, feature pyramids, and YOLOv3. Depthwise separable convolution is employed to design the backbone network for reducing parameters and for extracting crack features effectively. Then, the feature pyramid joins together low-resolution, semantically strong features at a high-resolution for obtaining rich semantics. Finally, YOLOv3 is used for the bounding box regression. YOLOv3-Lite is a fast and accurate crack detection method, which can be used on aircraft structure such as fuselage or engine blades. The result shows that, with almost no loss of detection accuracy, the speed of YOLOv3-Lite is 50\% more than that of YOLOv3. It can be concluded that YOLOv3-Lite can reach state-of-the-art performance.},
	language = {en},
	number = {18},
	urldate = {2023-04-18},
	journal = {Applied Sciences},
	author = {Li, Yadan and Han, Zhenqi and Xu, Haoyu and Liu, Lizhuang and Li, Xiaoqiang and Zhang, Keke},
	month = jan,
	year = {2019},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {YOLOv3, aircraft structure crack detection, depthwise separable convolution, feature pyramid},
	pages = {3781},
	file = {Full Text PDF:H\:\\Zotero\\storage\\F9BWIWAS\\Li et al. - 2019 - YOLOv3-Lite A Lightweight Crack Detection Network.pdf:application/pdf},
}

@inproceedings{liuApplicationYoloMask2021,
	title = {Application of {Yolo} on {Mask} {Detection} {Task}},
	doi = {10.1109/ICCRD51685.2021.9386366},
	abstract = {2020 has been a year marked by the COVID-19 pandemic. This event has caused disruptions to many aspects of normal life. An important aspect in reducing the impact of the pandemic is to control its spread. Studies have shown that one effective method in reducing the transmission of COVID-19 is to wear masks. Strict mask-wearing policies have been met with not only public sensation but also practical difficulty. We cannot hope to manually check if everyone on a street is wearing a mask properly. Existing technology to help automate mask checking uses deep learning models on real-time surveillance camera footages. The current dominant method to perform real-time mask detection uses Mask-R-CNN with ResNet as backbone. While giving good detection results, this method is computationally intensive and its efficiency in real-time face mask detection is not ideal. Our research proposes a new approach to the mask detection by replacing Mask-R-CNN with a more efficient model "YOLO" to increase the processing speed of real-time mask detection and not compromise on accuracy. Besides, given the small volume as well as extreme imbalance of the mask detection datasets, we adopt a latest progress made in few-shot visual classification, simple CNAPs, to improve the classification performance.},
	booktitle = {2021 {IEEE} 13th {International} {Conference} on {Computer} {Research} and {Development} ({ICCRD})},
	author = {Liu, Ren and Ren, Ziang},
	month = jan,
	year = {2021},
	keywords = {Computational modeling, Task analysis, Visualization, Real-time systems, Surveillance, COVID-19, Pandemics, YOLO, face detection, mask detection, Mask-RCNN},
	pages = {130--136},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\4U4SASVD\\9386366.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\DMXQWFWW\\Liu and Ren - 2021 - Application of Yolo on Mask Detection Task.pdf:application/pdf},
}

@inproceedings{zhangRealTimeDeepTransfer2021,
	title = {A {Real}-{Time} {Deep} {Transfer} {Learning} {Model} for {Facial} {Mask} {Detection}},
	doi = {10.1109/ICNS52807.2021.9441582},
	abstract = {With the rise of COVID-19, wearing masks in public areas has become paramount to preventing the transmission of the coronavirus, Furthermore, many public service providers may require customers to properly wear masks in order to be served. But current enforcement measures of mask mandates are mostly monitored by humans, and thus may be hard to execute effectively in densely populated and fast-moving venues like airports and public transit. An automated mask detection monitoring system could greatly reduce the required human labor. In this study, a facial mask detection software model, usable in existing surveillance applications such as airport monitoring systems, is developed using Keras, a high-level deep learning API, and TensorFlow, an end-to-end open source platform for machine learning. The model uses deep convolutional neural networks (DCNN), which were trained from a database of roughly 12,000 images of faces, with roughly a 50-50 split of masked and unmasked, with the goal of achieving binary classification to detect whether a person is wearing a mask. The DCNN mask detection software achieved a validation accuracy of 98 percent. We also used computer vision techniques from OpenCV including facial detection and contours to detect the location of the people’s faces. The DCNN was then paired with this application to achieve live feed object detection by first detecting a person, and then determine whether they are wearing a mask. Finally, we explore the potential implementation and applications of the facial detection model via the linkage of cameras in monitoring areas to signal and notify officials.As more countries are developing mask wearing regulations, automated masked face detection is a key real-world application [3]. Transportation systems are one area that have been considerably affected by the pandemic, as cities around the world had to enforce massive restrictions on public transport in order to limit transmission of the virus and ensure the safe passage of key workers during the emergency response [4]. Masks have since been required on planes, buses, trains, and other forms of public transportation traveling into, transportation hubs such as airports and stations [5].However, Machine Learning and Deep Learning can help to combat Covid-19 in many ways. Enabling researchers and clinicians to evaluate a vast amount of data to make various predictions. In particular, face mask detection may prove to be useful as a guide for government surveillance on people in public areas susceptible to transmission, to ensure that face mask regulations are upheld. To achieve this, deep learning binary classification facial mask model based on VGG19 and one-shot object detection methods will be trained.We contribute a dual face-and-mask detection system that can provide immediate feedback in live surveillance systems. The method introduced is fast and accurate and can work for live mask-checking in busy public areas, alleviating some of the human effort and error typically subject to the mask mandating task.},
	booktitle = {2021 {Integrated} {Communications} {Navigation} and {Surveillance} {Conference} ({ICNS})},
	author = {Zhang, Edward},
	month = apr,
	year = {2021},
	note = {ISSN: 2155-4951},
	keywords = {Deep learning, Cameras, Surveillance, COVID-19, Transfer learning, Airports, Software},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\EWG2ZW7Q\\9441582.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\7V42TNI7\\Zhang - 2021 - A Real-Time Deep Transfer Learning Model for Facia.pdf:application/pdf},
}

@inproceedings{youssryAccurateRealTimeFace2022,
	title = {Accurate {Real}-{Time} {Face} {Mask} {Detection} {Framework} {Using} {YOLOv5}},
	doi = {10.1109/DTS55284.2022.9809855},
	abstract = {After the COVID-19 pandemic, wearing a mask has become a must because it decreases the probability of infection by 68\%. That is why a fast and accurate automatic mask detection is crucial to public institutions. In this paper, we present an accurate framework for real-time mask detection using YOLOv5 object detection algorithm. Our framework consists of four stages: image preprocessing by normalization and adding noise, adding negative samples and data augmentation then the detection core based on a modified version of YOLOv5. The proposed framework achieves 95.9\% precision and 84.8\% mean average precision using the Face Mask Detection dataset with a 10 milliseconds inference time.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Design} \& {Test} of {Integrated} {Micro} \& {Nano}-{Systems} ({DTS})},
	author = {Youssry, Nouran and Khattab, Ahmed},
	month = jun,
	year = {2022},
	keywords = {Deep learning, Object detection, Real-time systems, COVID-19, Pandemics, face mask detection, Faces, data augmentation, You Only Look Once (YOLO)},
	pages = {01--06},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\CRWY87H4\\9809855.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\UJJMWJ8H\\Youssry and Khattab - 2022 - Accurate Real-Time Face Mask Detection Framework U.pdf:application/pdf},
}

@misc{EvaluationYOLOXMobileNetV2,
	title = {Evaluation of {YOLO}-{X} and {MobileNetV2} as {Face} {Mask} {Detection} {Algorithms}},
	url = {https://ieeexplore-ieee-org.oca.rizal.library.remotexs.co/document/9951831/},
	abstract = {Even though COVID-19 still exists, people are more reluctant to wear masks in public places, in fact only 73\% of Indonesian still do. Hence, automatic mask surveillance in public places is still needed. In this paper, we compare two algorithms named YOLO-X and MobileNetV2 to detect face masks. YOLO-X was able to outperform other YOLO algorithms in object detection. While, according to researchers, MobileNetV2 achieved 9S\% in face mask detection. To fairly evaluate both algorithms we need to conduct research under controlled variables including using the same datasets and devices. We used public datasets which consists of 1493 mask images and 6451 non mask images for training and testing. The results show that YOLO-X outperforms MobileNetV2 as it achieves 95.0\%, 98.7\%, 93.7\%, and 96.1\% for accuracy, average precision, recall, and F1-score respectively. YOLO-X also performs better in detecting faces with occlusion such as glasses, hands, and postures than MobileNetV2. However, YOLO-X detects faces and face masks 31.9\% slower than MobileNetV2.},
	language = {en-US},
	urldate = {2023-04-18},
	file = {Snapshot:H\:\\Zotero\\storage\\PCXZZNT2\\9951831.html:text/html},
}

@inproceedings{xueIntelligentDetectionRecognition2020,
	title = {Intelligent detection and recognition system for mask wearing based on improved {RetinaFace} algorithm},
	doi = {10.1109/MLBDBI51377.2020.00100},
	abstract = {The COVID-19 can be transmitted through airborne droplets, aerosols and other carriers. In order to better reduce people's risk of infection, individuals need to wear masks to prevent the spread of the virus when going out to public places, seeking medical treatment, and taking public transportation. This paper is based on the improved RETINAFACE algorithm, which effectively realizes the detection of mask wearing, and on the basis of this algorithm, realizes the function of judging whether the mask is worn correctly. In the face recognition algorithm, this paper designs a face recognition algorithm with higher accuracy. The system combines a face mask wearing detection algorithm, a mask standard wearing detection algorithm and a face recognition algorithm. In addition, this article adds a voice prompt module to better assist the integrity of the system's functions. The test results of the final experiment show that the system can effectively achieve the purpose of face mask detection and recognition.},
	booktitle = {2020 2nd {International} {Conference} on {Machine} {Learning}, {Big} {Data} and {Business} {Intelligence} ({MLBDBI})},
	author = {Xue, Bin and Hu, Jianpeng and Zhang, Pengming},
	month = oct,
	year = {2020},
	keywords = {Standards, Face recognition, Detection algorithms, Face mask detection, face recognition, Machine learning algorithms, mask standard wearing detection, Medical treatment, Public transportation, RetinaFace algorithm, Viruses (medical)},
	pages = {474--479},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\Z8CS8BUU\\9361033.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\WNC8X43M\\Xue et al. - 2020 - Intelligent detection and recognition system for m.pdf:application/pdf},
}

@inproceedings{sathyamurthyRealtimeFaceMask2021,
	title = {Realtime {Face} {Mask} {Detection} {Using} {TINY}-{YOLO} {V4}},
	doi = {10.1109/ICCCT53315.2021.9711838},
	abstract = {In the time of the Covid-19 pandemic there is a need to maintain social distancing and prioritize personal hygiene by the use of face masks and proper sanitary precautions. This although is hard to be monitored and controlled accurately and efficiently, can be done through the use of object detection using convolutional neural networks. This can be done in a way using Tiny-YOLOv4 which is an object detection algorithm that provides lightning-fast detection for many classes of objects without the use of such hardware resources. This project aims to train and test a custom data set using this algorithm to create a highly efficient and accurate face mask detection system that can be easily customized to add additional features such as warning systems, etc. It aims to be a system that can prove to be useful once the pandemic is over as it provides crucial data for the prevention and control of any other possible pandemics that may occur in the future.},
	booktitle = {2021 4th {International} {Conference} on {Computing} and {Communications} {Technologies} ({ICCCT})},
	author = {Sathyamurthy, Kavi Varun and Shri Rajmohan, A.R. and Ram Tejaswar, A. and V, Kavitha and Manimala, G.},
	month = dec,
	year = {2021},
	keywords = {Feature extraction, Object detection, Convolutional Neural Networks, Human factors, Social factors, Pandemics, Machine Learning, Hardware, Communications technology, Object Detection},
	pages = {169--174},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\NJEPS46J\\9711838.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\UPCXUFSS\\Sathyamurthy et al. - 2021 - Realtime Face Mask Detection Using TINY-YOLO V4.pdf:application/pdf},
}

@inproceedings{dangeFaceMaskDetection2022,
	title = {Face {Mask} {Detection} under the {Threat} of {Covid}-19 {Virus}},
	doi = {10.1109/ICBDS53701.2022.9935853},
	abstract = {For wearing a mask, Covid19 has a new identity. Detecting masked faces with accuracy and efficiency is becoming increasingly important. Face mask recognition is a tough task because of the high occlusions that cause the loss of face in-formation. Furthermore, There aren't many large-scale, masked face with a well-label datasets out there, which makes face mask recognition more difficult. CNN-based deep learning algorithms have achieved significant advances in a variety, one aspect of computer vision is face detection disciplines. We introduce a new cascade structure based on CNN for recognising face masks in this paper, This is made up of three convolutional neural networks that have been successfully built. To finetune our CNN models, we propose a new dataset named “facial mask dataset.” due to a paucity of face masked training instances. Face masks were tested on a set of faces, we assess our suggested face mask recognition algorithm, and it performs admirably. A new cascade framework based on CNN is proposed for detecting face masks and their types, which comprises of three properly built convolutional neural networks.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Blockchain} and {Distributed} {Systems} {Security} ({ICBDS})},
	author = {Dange, B.J. and Khalate, S.S. and Kshirsagar, D.B. and Gunjal, S.N. and Khodke, H.E. and Bhaskar, T.},
	month = sep,
	year = {2022},
	keywords = {Feature extraction, Training, Convolutional neural networks, Face detection, Convolutional Neural Networks, Mask Detection, COVID-19, Face recognition, Computer viruses, Feature Extraction, Image Processing, OpenCv, TensorFlow},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\QBBH7NWF\\9935853.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\WP93VW2L\\Dange et al. - 2022 - Face Mask Detection under the Threat of Covid-19 V.pdf:application/pdf},
}

@inproceedings{alayaryFaceMaskedUnmasked2022,
	title = {Face {Masked} and {Unmasked} {Humans} {Detection} and {Tracking} in {Video} {Surveillance}},
	doi = {10.1109/NILES56402.2022.9942375},
	abstract = {Due to the spread of COVID-19, people wearing face masks became a regular occurrence worldwide. Moreover, there are nations where covering one's face is done for religious or cultural reasons, or even wear face masks for convenience. However, current face detection and tracking systems are hindered by face masks as the full facial features are no longer visible and therefore became less effective. In this paper, it is proposed to improve current face detection and long-term tracking technology by extracting the facial features of the top regions of the face, taking into account the eye, eyebrow, and forehead. The methodology contains two models, the face detector and the long-term object tracker. The face detection model uses a joint dataset from ISL-UFMD and MaskedFace-Net. The dataset is used to train a Keras sequential model. The object detection model uses pre-trained YOLOv4 weights and DeepSORT to identify people and uses the tracking-by-detection method to perform long-term tracking throughout the surveillance video. The final face detection model results show a testing accuracy of 93.33\% and a loss of 26.92\%, which are up to par and comparable with other state-of-the-art models.},
	booktitle = {2022 4th {Novel} {Intelligent} and {Leading} {Emerging} {Sciences} {Conference} ({NILES})},
	author = {Alayary, Yomna and Shoukry, Nadeen and El Ghany, Mohamed A. Abd and Salem, Mohammed A.-M.},
	month = oct,
	year = {2022},
	keywords = {Feature extraction, Detectors, Object detection, Video surveillance, Object recognition, COVID-19, Cultural differences, DeepSORT, face tracking, ISL-UFMD, Keras sequential model, masked face detection, MaskedFace-Net, YOLOv4},
	pages = {211--215},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\5AVYFAUM\\9942375.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\MGRRC4XJ\\Alayary et al. - 2022 - Face Masked and Unmasked Humans Detection and Trac.pdf:application/pdf},
}

@inproceedings{dinhMaskedFaceDetection2022,
	title = {Masked {Face} {Detection} with {Illumination} {Awareness}},
	doi = {10.1109/ISMICT56646.2022.9828233},
	abstract = {Mask mandate has been applied in many countries in the last two years as a simple but effective way to limit the Covid-19 transmission. Besides the guidance from authorities regarding mask use in public, numerous vision-based approaches have been developed to aid with the monitoring of face mask wearing. Despite promising results have been obtained, several challenges in vision-based masked face detection still remain, primarily due to the insufficient of a quality dataset covering adequate variations in lighting conditions, object scales, mask types, or occlusion levels. In this paper, we investigate the effectiveness of a lightweight masked face detection system under different lighting conditions and the possibility of enhancing its performance with the employment of an image enhancement algorithm and an illumination awareness classifier. A dataset of human subjects with and without face masks in different lighting conditions is first introduced. An illumination awareness classifier is then trained on the collected dataset, the labeling of which is processed automatically based on the difference in detection accuracy when an image enhancement algorithm is taken into account. Experimental results have shown that the combination of the masked face detection system with the illumination awareness and an image enhancement algorithm can boost the system performance to up to 8.6\%, 7.4\%, and 8.5\% in terms of Accuracy, F1-score, and AP-M, respectively.},
	booktitle = {2022 {IEEE} 16th {International} {Symposium} on {Medical} {Information} and {Communication} {Technology} ({ISMICT})},
	author = {Dinh, Tran Hiep and Doan, Quang Manh and Trung, Nguyen Linh and Nguyen, Diep N. and Lin, Chin-Teng},
	month = may,
	year = {2022},
	note = {ISSN: 2326-8301},
	keywords = {Classification algorithms, System performance, Face detection, Lighting, Employment, masked face detection, Covid-19, Labeling, low-illumination image enhancement, Measurement},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\6V89IRUF\\9828233.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\C9BZV5N9\\Dinh et al. - 2022 - Masked Face Detection with Illumination Awareness.pdf:application/pdf},
}

@inproceedings{hanMaskDetectionMethod2020,
	title = {A {Mask} {Detection} {Method} for {Shoppers} {Under} the {Threat} of {COVID}-19 {Coronavirus}},
	doi = {10.1109/CVIDL51233.2020.00-54},
	abstract = {Object detection, which aims to automatically mark the coordinates of objects of interest in pictures or videos, is an extension of image classification. In recent years, it has been widely used in intelligent traffic management, intelligent monitoring systems, military object detection, and surgical instrument positioning in medical navigation surgery, etc. COVID-19, a novel coronavirus outbreak at the end of 2019, poses a serious threat to public health. Many countries require everyone to wear a mask in public to prevent the spread of coronavirus. To effectively prevent the spread of the coronavirus, we present an object detection method based on single-shot detector (SSD), which focuses on accurate and real-time face masks detection in the supermarket. We make contributions in the following three aspects: 1) presenting a lightweight backbone network for feature extraction, which based on SSD and spatial separable convolution, aiming to improve the detection speed and meet the requirements of real-time detection; 2) proposing a Feature Enhancement Module (FEM) to strengthen the deep features learned from CNN models, aiming to enhance the feature representation of the small objects; 3) constructing COVID-19Mask, a large-scale dataset to detect whether shoppers are wearing masks, by collecting images in two supermarkets. The experiment results illustrate the high detection precision and real-time performance of the proposed algorithm.},
	booktitle = {2020 {International} {Conference} on {Computer} {Vision}, {Image} and {Deep} {Learning} ({CVIDL})},
	author = {Han, Wenxuan and Huang, Zitong and kuerban, Alifu. and Yan, Meng and Fu, Haitang},
	month = jul,
	year = {2020},
	keywords = {Feature extraction, Training, Object detection, Convolution, COVID-19, feature fusion, Finite element analysis, Kernel, masks, spatial separable convolution},
	pages = {442--447},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\LZNKH43J\\9270548.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\AD3DZLCD\\Han et al. - 2020 - A Mask Detection Method for Shoppers Under the Thr.pdf:application/pdf},
}

@inproceedings{indulkarAlleviationCOVIDMeans2021,
	title = {Alleviation of {COVID} by means of {Social} {Distancing} \& {Face} {Mask} {Detection} {Using} {YOLO} {V4}},
	doi = {10.1109/ICCICT50803.2021.9510168},
	abstract = {This paper consists of social distancing \& face mask detection for the events of coronavirus, alleviation in such pandemic can be solved by social distancing as well as putting on a face mask. This small step of wearing a face mask as well as following social distancing would save lots of lives as the spread of the virus could be mitigated. YOLO stands for You Only Look Once, this algorithm is used for Object Detection as well as Object Tracking, this research uses YOLO for calculating the social distancing \& identifying face mask on people's face with the help of Object Detection, whereas tracking the face is done by Object Tracking. The minimum distance to keep while adhering for social distancing is 6 Feet, keeping this as the base for calculating distance, the model was trained and used for object detection as well as for object tracking. There are different types of algorithms available, YOLO stands out from all the other present currently. The custom datasets were used for the understanding the face masks and it was trained on those datasets for detection and tracking. For evaluation of the trained model, mAP (Mean Average Precision) was calculated for both the use cases (Social Distancing \& Face Mask Detection), it works by comparing the ground-truth bounding box vs the detected box and, in the end, returns the score. The higher the mAP score would be, the better model is in the detection of objects. Mean Average Precision was calculated for two different thresholds (0.25 \% \& 0.50 \%) with 101 recall points. Three different classes were created for classification those were Good, Bad \& None, for which True Positive \& False Positive values were calculated with ROC Curve for better understanding.},
	booktitle = {2021 {International} {Conference} on {Communication} information and {Computing} {Technology} ({ICCICT})},
	author = {Indulkar, Yash},
	month = jun,
	year = {2021},
	keywords = {Object detection, Real-time systems, Human factors, Social Distancing, COVID-19, Face Mask Detection, Social factors, Pandemics, YOLO, Object Detection, Coronavirus, Object Tracking, Predictive models},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\SXJPD8QC\\9510168.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\SLR6PJFE\\Indulkar - 2021 - Alleviation of COVID by means of Social Distancing.pdf:application/pdf},
}

@inproceedings{srinivasanCOVID19MonitoringSystem2021,
	title = {{COVID}-19 {Monitoring} {System} using {Social} {Distancing} and {Face} {Mask} {Detection} on {Surveillance} video datasets},
	doi = {10.1109/ESCI50559.2021.9396783},
	abstract = {In the current times, the fear and danger of COVID-19 virus still stands large. Manual monitoring of social distancing norms is impractical with a large population moving about and with insufficient task force and resources to administer them. There is a need for a lightweight, robust and 24X7 video-monitoring system that automates this process. This paper proposes a comprehensive and effective solution to perform person detection, social distancing violation detection, face detection and face mask classification using object detection, clustering and Convolution Neural Network (CNN) based binary classifier. For this, YOLOv3, Density-based spatial clustering of applications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and MobileNetV2 based binary classifier have been employed on surveillance video datasets. This paper also provides a comparative study of different face detection and face mask classification models. Finally, a video dataset labelling method is proposed along with the labelled video dataset to compensate for the lack of dataset in the community and is used for evaluation of the system. The system performance is evaluated in terms of accuracy, F1 score as well as the prediction time, which has to be low for practical applicability. The system performs with an accuracy of 91.2\% and F1 score of 90.79\% on the labelled video dataset and has an average prediction time of 7.12 seconds for 78 frames of a video.},
	booktitle = {2021 {International} {Conference} on {Emerging} {Smart} {Computing} and {Informatics} ({ESCI})},
	author = {Srinivasan, Sahana and Rujula Singh, R and Biradar, Ruchita R and Revathi, SA},
	month = mar,
	year = {2021},
	keywords = {Face detection, Surveillance, Human factors, MobileNetV2, Social Distancing, COVID-19, Social factors, Face recognition, Viruses (medical), Convolution Neural Networks, Density-based spatial clustering of applications with noise, Dual Shot Face Detector, Face Mask Classification, Video surveillance., You Only Look Once},
	pages = {449--455},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\VR5UERIX\\9396783.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\DIA6TNPX\\Srinivasan et al. - 2021 - COVID-19 Monitoring System using Social Distancing.pdf:application/pdf},
}

@inproceedings{lathasoniyaRealTimeSafe2022,
	title = {Real {Time} {Safe} {Social} {Distance} and {Face} {Mask} {Detection} using {YOLO} {Algorithm} and {Deep} {Learning}},
	doi = {10.1109/ICICT54344.2022.9850507},
	abstract = {The pandemic hit the world, and governments all around the world adopted drastic but necessary steps to stop it from spreading may be COVID or other versions. It’s difficult to ensure that individuals follow these important social distance principles in large institutions. An automated system is necessary to enable easy tracking of such offenders. As a result, this system was designed to identify specific infractions in real-time. The proposed system’s initial use is to recognize faces of people to decide whether they are wearing an approved mask or not. The other use is to evaluate whether the social distance is maintained between two people in the most efficient manner that is very accurate, and easy way possible, requiring the least amount of effort from supervisory authorities.},
	booktitle = {2022 {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {Latha Soniya, Pushpa and Velayutham, R},
	month = jul,
	year = {2022},
	note = {ISSN: 2767-7788},
	keywords = {Deep learning, Visualization, Real-time systems, Cameras, Convolutional neural network, Human factors, Social factors, Pandemics, Mask, Pandemic, Social distance, Stop spreading},
	pages = {528--532},
	annote = {GUD!

},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\KGNW7DE8\\9850507.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\F7VJKAR5\\Latha Soniya and Velayutham - 2022 - Real Time Safe Social Distance and Face Mask Detec.pdf:application/pdf},
}

@inproceedings{rSocialDistancingFace2022,
	title = {Social {Distancing} and {Face} {Mask} {Detection} {For} {Covid} {Prevention}},
	doi = {10.1109/ICSSS54381.2022.9782283},
	abstract = {The novel virus additionally carries Covid-19 which has a profound outcome of numerous sections in lots of regions and public sectors are growing around the arena to sell viral media coverage. This is a small step to carrying a face mask, following far from the network will keep many lives because the unfolding of the virus novel may be reduced. This article carries records on public dissemination and face protecting approximately instances of sicknesses consisting of the radical virus that may be resolved through worrying for its social distribution/management. This became used to provide Mask Detecition the use of OrenCV, Keras/TensorFlow, and Deer Learning. This application may be effortlessly integrated and established on a whole lot of high-density embedded gadgets with the use of MobileNetV2 architecture. The gadget will come across faces mask on various instantaneous videos, many photos and images.},
	booktitle = {2022 8th {International} {Conference} on {Smart} {Structures} and {Systems} ({ICSSS})},
	author = {R, Aneesh. and K, Prasanth. and Madhavan, P.},
	month = apr,
	year = {2022},
	keywords = {Visualization, deep learning, Human factors, OpenCV, COVID-19, Social factors, CNN, Coronaviruses, face masks awareness, Internet, Keras/TensorFlow, Media, social distancing detection},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\ZNTEP26P\\9782283.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\X2GED4B5\\R et al. - 2022 - Social Distancing and Face Mask Detection For Covi.pdf:application/pdf},
}

@inproceedings{bhambaniRealtimeFaceMask2020,
	title = {Real-time {Face} {Mask} and {Social} {Distancing} {Violation} {Detection} {System} using {YOLO}},
	doi = {10.1109/B-HTC50970.2020.9297902},
	abstract = {With the recent outbreak and rapid transmission of the COVID-19 pandemic, the need for the public to follow social distancing norms and wear masks in public is only increasing. According to the World Health Organization, to follow proper social distancing, people in public places must maintain at least 3ft or 1m distance between each other. This paper focuses on a solution to help enforce proper social distancing and wearing masks in public using YOLO object detection on video footage and images in real time. The experimental results shown in this paper infer that the detection of masked faces and human subjects based on YOLO has stronger robustness and faster detection speed as compared to its competitors. Our proposed object detection model achieved a mean average precision score of 94.75\% with an inference speed of 38 FPS on video. The network ensures inference speed capable of delivering real-time results without compromising on accuracy, even in complex setups. The social distancing method proposed also yields promising results in several variable scenarios.},
	booktitle = {2020 {IEEE} {Bangalore} {Humanitarian} {Technology} {Conference} ({B}-{HTC})},
	author = {Bhambani, Krisha and Jain, Tanmay and Sultanpure, Kavita A.},
	month = oct,
	year = {2020},
	keywords = {Detectors, Real-time systems, Cameras, Human factors, Social Distancing, COVID-19, Social factors, YOLO, Faces, Diseases, Masks, Real-time},
	pages = {1--6},
	annote = {GUD!
},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\DFYJSXJM\\9297902.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\5MIIL22J\\Bhambani et al. - 2020 - Real-time Face Mask and Social Distancing Violatio.pdf:application/pdf},
}

@article{liYOLOv3LiteLightweightCrack2019a,
	title = {{YOLOv3}-{Lite}: {A} {Lightweight} {Crack} {Detection} {Network} for {Aircraft} {Structure} {Based} on {Depthwise} {Separable} {Convolutions}},
	volume = {9},
	issn = {2076-3417},
	shorttitle = {{YOLOv3}-{Lite}},
	url = {https://www.mdpi.com/2076-3417/9/18/3781},
	doi = {10.3390/app9183781},
	abstract = {Due to the high proportion of aircraft faults caused by cracks in aircraft structures, crack inspection in aircraft structures has long played an important role in the aviation industry. The existing approaches, however, are time-consuming or have poor accuracy, given the complex background of aircraft structure images. In order to solve these problems, we propose the YOLOv3-Lite method, which combines depthwise separable convolution, feature pyramids, and YOLOv3. Depthwise separable convolution is employed to design the backbone network for reducing parameters and for extracting crack features effectively. Then, the feature pyramid joins together low-resolution, semantically strong features at a high-resolution for obtaining rich semantics. Finally, YOLOv3 is used for the bounding box regression. YOLOv3-Lite is a fast and accurate crack detection method, which can be used on aircraft structure such as fuselage or engine blades. The result shows that, with almost no loss of detection accuracy, the speed of YOLOv3-Lite is 50\% more than that of YOLOv3. It can be concluded that YOLOv3-Lite can reach state-of-the-art performance.},
	language = {en},
	number = {18},
	urldate = {2023-04-19},
	journal = {Applied Sciences},
	author = {Li, Yadan and Han, Zhenqi and Xu, Haoyu and Liu, Lizhuang and Li, Xiaoqiang and Zhang, Keke},
	month = sep,
	year = {2019},
	pages = {3781},
	file = {Full Text:H\:\\Zotero\\storage\\K8EZW84J\\Li et al. - 2019 - YOLOv3-Lite A Lightweight Crack Detection Network.pdf:application/pdf},
}

@inproceedings{chakarDepthwiseSeparableConvolutions2020a,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Depthwise {Separable} {Convolutions} and {Variational} {Dropout} within the context of {YOLOv3}},
	isbn = {978-3-030-64556-4},
	doi = {10.1007/978-3-030-64556-4_9},
	abstract = {Deep learning algorithms have demonstrated remarkable performance in many sectors and have become one of the main foundations of modern computer-vision solutions. However, these algorithms often impose prohibitive levels of memory and computational overhead, especially in resource-constrained environments. In this study, we combine the state-of-the-art object-detection model YOLOv3 with depthwise separable convolutions and variational dropout in an attempt to bridge the gap between the superior accuracy of convolutional neural networks and the limited access to computational resources. We propose three lightweight variants of YOLOv3 by replacing the original network’s standard convolutions with depthwise separable convolutions at different strategic locations within the network, and we evaluate their impacts on YOLOv3’s size, speed, and accuracy. We also explore variational dropout: a technique that finds individual and unbounded dropout rates for each neural network weight. Experiments on the PASCAL VOC benchmark dataset show promising results where variational dropout combined with the most efficient YOLOv3 variant lead to an extremely sparse solution that reduces 95\% of the baseline network’s parameters at a relatively small drop of 3\% in accuracy.},
	language = {en},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Chakar, Joseph and Sobbahi, Rayan Al and Tekli, Joe},
	editor = {Bebis, George and Yin, Zhaozheng and Kim, Edward and Bender, Jan and Subr, Kartic and Kwon, Bum Chul and Zhao, Jian and Kalkofen, Denis and Baciu, George},
	year = {2020},
	keywords = {Object Detection, Computer Vision, Convolutional Neural Network, Depthwise Separable Convolution, Network Sparsification, Variational Dropout},
	pages = {107--120},
}

@article{xuEfficientPedestrianDetection2022a,
	title = {An {Efficient} {Pedestrian} {Detection} for {Realtime} {Surveillance} {Systems} {Based} on {Modified} {YOLOv3}},
	volume = {6},
	issn = {2469-7281},
	doi = {10.1109/JRFID.2022.3212907},
	abstract = {Pedestrian detection is an important branch of object detection due to its various applications. It plays a vital role in many fields such as intelligent surveillance systems. The recognition, identification and tracking modules of surveillance are based on efficient and accurate pedestrian detection. Our paper proposes an efficient model to solve real-time pedestrian detection with high accuracy based on modified ShuffleNet and YOLOv3 models. We provide a method to pick the dimensions and number of anchor boxes for predicting bounding boxes accurately. Then we use two improved shuffle units to lightweight the backbone of YOLOv3, which reduces the 67.5\% floating point operations per second (FLOPs) and 65.1\% parameters. We validate our model on CrowdHuman detection data set and get 62.7 mAP for face and 62.0 mAP person with 0.748 average IOU. Our network processes images in real-time at 186.1 frames per second for network and 12.5 frames per second for the entire model on CrowdHuman.},
	journal = {IEEE Journal of Radio Frequency Identification},
	author = {Xu, Ming and Wang, Zhen and Liu, Xingmao and Ma, Longhua and Shehzad, Ahsan},
	year = {2022},
	note = {Conference Name: IEEE Journal of Radio Frequency Identification},
	keywords = {Convolutional neural networks, Computational modeling, Object detection, Optimization, Real-time systems, YOLOv3, Computer vision, Convolutional neural network, K-means pedestrian detection, Radiofrequency identification, shuffle unit video surveillance},
	pages = {972--976},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\2GBKJG2P\\9913649.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\5DLL5H7C\\Xu et al. - 2022 - An Efficient Pedestrian Detection for Realtime Sur.pdf:application/pdf},
}

@inproceedings{luYOLOcompactEfficientYOLO2020,
	title = {{YOLO}-compact: {An} {Efficient} {YOLO} {Network} for {Single} {Category} {Real}-time {Object} {Detection}},
	shorttitle = {{YOLO}-compact},
	doi = {10.1109/CCDC49329.2020.9164580},
	abstract = {In practical applications, the number of category in object detection is always single. In this paper, an efficient YOLO-compact network designed for single category real-time object detection is proposed. This paper first explored a series of methods for converting a large and deep network to a compact and efficient network, through a series of ablation experiments. Then these methods were assembled to YOLOv3 network, which obtains the YOLO-compact's infrastructure. A network structure design approach that separates the down sampling layer from all network modules was proposed, which facilitates the modular design of the network. The shortcut structure in the RFB module is changed to the direct connection structure, and the 1×3+3×1+3×3 convolution structure is used instead of 5×5 convolution, which obtains efficient RFB-c module. The residual bottleneck block has been improved, by removing the last 1×1 convolution layer and using 3×3 depth wise separable convolution. Since pedestrian is the most representative object in practical applications, this paper uses the result on person category in VOC2007 test set to represent the network performance. The model size of YOLO-compact is only 9MB, which is 3.7 times smaller than tiny-yolov3, 6.7 times smaller than tiny-yolov2, and 26 times smaller than YOLOv3. The AP result of YOLO-compact is 86.85\%, which is 37\% higher than tiny-yolov3, 32\% higher than tiny-yolov2, and even 2.7\% higher than the YOLOv3.},
	booktitle = {2020 {Chinese} {Control} {And} {Decision} {Conference} ({CCDC})},
	author = {Lu, Yonghui and Zhang, Langwen and Xie, Wei},
	month = aug,
	year = {2020},
	note = {ISSN: 1948-9447},
	keywords = {Feature extraction, Standards, Training, Object detection, Convolution, Real-time systems, Object Detection, Kernel, Efficient, YOLO-compact},
	pages = {1931--1936},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\JGF8CJHV\\9164580.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\59EW2HKP\\Lu et al. - 2020 - YOLO-compact An Efficient YOLO Network for Single.pdf:application/pdf},
}

@inproceedings{bohongGarbageDetectionAlgorithm2022,
	title = {Garbage {Detection} {Algorithm} {Based} on {YOLO} v3},
	doi = {10.1109/EEBDA53927.2022.9744738},
	abstract = {In recent years, garbage classification has become the focus of the world. YOLOv3 has good real-time performance and accurate detection accuracy, which can meet the requirements of garbage sorting equipment. In this paper, we introduce a lightweight cross-channel interaction Attention mechanism in the residual unit of YOLOv3, and propose a new target detection algorithm, namely Efficient Channel Attention YOLO (ECA-YOLO) detection algorithm. So the target detection algorithm can get more complete and more effective feature information. Experimental results show that the improved model performs better than YOLOv3 in the garbage image data set of “Huawei Garbage Classification Challenge Cup”. The improved model has a high accuracy and recall rate, and the detection mAP is improved by 1.07\%, while the detection speed is kept unchanged and the number of parameters is introduced.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Electrical} {Engineering}, {Big} {Data} and {Algorithms} ({EEBDA})},
	author = {Bohong, Liu and Xinpeng, Wang},
	month = feb,
	year = {2022},
	keywords = {Feature extraction, Object detection, Data models, Real-time systems, Conferences, deep learning, object detection, Costs, Electrical engineering, garbage classification, YOLOv3 algorithm},
	pages = {784--788},
	file = {IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\68EG7USB\\Bohong and Xinpeng - 2022 - Garbage Detection Algorithm Based on YOLO v3.pdf:application/pdf},
}

@inproceedings{liFireObjectDetection2022,
	title = {Fire {Object} {Detection} {Algorithm} {Based} on {Improved} {YOLOv3}-tiny},
	doi = {10.1109/ICCCBDA55098.2022.9778892},
	abstract = {In view of the poor performance of the commonly used network models on the self-made fire dataset and the real-time requirements for the fire detection, we propose an improved fire object detection model based on the YOLOv3-tiny lightweight network. The specific measures are as follows: Replace the first four pooling layers in the backbone network with the 3{\textbackslash}times 3 convolutional layers with a step size of 2, increasing network nonlinearity while reducing the information loss caused by the pooling layer; On the basis of the original network detection layer, a new detection layer of 52{\textbackslash}times 52 size is added to improve the detection effect of small object fire; In order to reduce the detection errors caused by the complex background of the dataset, the SE channel attention module is added after the fusion of different detection layers to strengthen the attention to important features. According to the experimental results, the improved YOLOv3-tiny network has a mean average precision rate of 80.8\%, which is 5.9\% higher than the original network, which fully proves the effectiveness of the model.},
	booktitle = {2022 7th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} {Analytics} ({ICCCBDA})},
	author = {Li, Yongquan and Rong, Leilei and Li, Runqing and Xu, Yan},
	month = apr,
	year = {2022},
	keywords = {Feature extraction, Computational modeling, Object detection, deep learning, fire detection, improved YOLOv3-tiny, Interference, Loss measurement, Measurement uncertainty, SE module, Size measurement},
	pages = {264--269},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\XYZJR84Z\\9778892.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\V5LKYEGC\\Li et al. - 2022 - Fire Object Detection Algorithm Based on Improved .pdf:application/pdf},
}

@inproceedings{gongObjectDetectionBased2019,
	title = {Object {Detection} {Based} on {Improved} {YOLOv3}-tiny},
	doi = {10.1109/CAC48633.2019.8996750},
	abstract = {Deep learning has gradually become the mainstream object detection algorithm because of its powerful feature extraction ability and adaptive ability. However, how to guarantee the accuracy and speed is still a huge challenge in the field of object detection. This paper proposes an improved YOLOv3-tiny for object detection based on the idea of feature fusion. YOLOv3-tiny is chosen as the basic network framework to ensure the identification speed. In order to improve the poor detection accuracy of YOLOv3-tiny network, feature fusion is carried out based on Feature Pyramid Network. The last DBL (Darknetconv2d\_ Batch Normalization\_Leaky) output of the second scale output layer is fused with the fourth DBL output of the network. And a 52{\textbackslash}times 52 scale output is added on the basis of the original network. Experiments were carried out on our dataset. The experimental results show that compared with YOLOv3-tiny, the accuracy of the improved network structure is increased by 6.3\%, and the detection speed is 31.8fps, ensuring the requirements of real-time detection.},
	booktitle = {2019 {Chinese} {Automation} {Congress} ({CAC})},
	author = {Gong, Hua and Li, Hui and Xu, Ke and Zhang, Yong},
	month = nov,
	year = {2019},
	note = {ISSN: 2688-0938},
	keywords = {Feature extraction, Training, Object detection, Convolution, Real-time systems, object detection, Computer architecture, anchor cluster, improved YOLOv3-Tiny, Microprocessors},
	pages = {3240--3245},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\8NPPL78N\\8996750.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\5PFLGPYK\\Gong et al. - 2019 - Object Detection Based on Improved YOLOv3-tiny.pdf:application/pdf},
}

@inproceedings{gongPedestrianDetectionAlgorithm2022,
	title = {Pedestrian {Detection} {Algorithm} {Based} on {YOLOv3}},
	doi = {10.1109/AIAM57466.2022.00139},
	abstract = {The current YOLOv3 algorithm also has shortcomings in dealing with pedestrian detection problems, such as pedestrian missed detection, false detection, and pedestrian multi-pose change problems. Therefore, an improved algorithm based on the YOLOv3 algorithm for accurate pedestrian detection in images was proposed. The algorithm introduces the CSPNet structure and the ECA attention mechanism to the original YOLOv3 algorithm, which greatly reduces the amount of computation compared with the original algorithm, and the performance is significantly improved. The experiment in this paper is based on the INRIA dataset,and the experimental results show that the detection performance of the optimized algorithm is better.Compared with the original YOLOv3 network, the optimized algorithm improves the mAP value by 93.89\%.},
	booktitle = {2022 4th {International} {Conference} on {Artificial} {Intelligence} and {Advanced} {Manufacturing} ({AIAM})},
	author = {Gong, Xueli and Xie, Mingliang and Xu, Haosen},
	month = oct,
	year = {2022},
	keywords = {Convolutional neural networks, Pedestrian Detection, YOLOv3, Detection algorithms, Artificial intelligence, Complexity theory, CSPNet, ECA, Manufacturing, Prediction algorithms, Redundancy},
	pages = {686--689},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\RYGWCKMS\\10071374.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\X3EIX5HK\\Gong et al. - 2022 - Pedestrian Detection Algorithm Based on YOLOv3.pdf:application/pdf},
}

@misc{WhatPyTorch,
	title = {What is {PyTorch}?},
	url = {https://www.nvidia.com/en-us/glossary/data-science/pytorch/},
	abstract = {Learn all about PyTorch and more.},
	language = {en-us},
	urldate = {2023-04-28},
	journal = {NVIDIA Data Science Glossary},
	note = {Retrieved April 28, 2023 from https://www.nvidia.com/en-us/glossary/data-science/pytorch/},
	file = {Snapshot:H\:\\Zotero\\storage\\H6IU2MCR\\pytorch.html:text/html},
}

@misc{howardSearchingMobileNetV32019,
	title = {Searching for {MobileNetV3}},
	url = {http://arxiv.org/abs/1905.02244},
	doi = {10.48550/arXiv.1905.02244},
	abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2{\textbackslash}\% more accurate on ImageNet classification while reducing latency by 15{\textbackslash}\% compared to MobileNetV2. MobileNetV3-Small is 4.6{\textbackslash}\% more accurate while reducing latency by 5{\textbackslash}\% compared to MobileNetV2. MobileNetV3-Large detection is 25{\textbackslash}\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30{\textbackslash}\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	month = nov,
	year = {2019},
	note = {arXiv:1905.02244 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2019},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\CHGAEVEN\\Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\MLSW532D\\1905.html:text/html},
}

@misc{shangUnderstandingImprovingConvolutional2016,
	title = {Understanding and {Improving} {Convolutional} {Neural} {Networks} via {Concatenated} {Rectified} {Linear} {Units}},
	url = {http://arxiv.org/abs/1603.05201},
	doi = {10.48550/arXiv.1603.05201},
	abstract = {Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
	month = jul,
	year = {2016},
	note = {arXiv:1603.05201 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ICML 2016},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\4D3J8S79\\Shang et al. - 2016 - Understanding and Improving Convolutional Neural N.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\Q4E7TYSP\\1603.html:text/html},
}

@misc{FaceMaskDetectiona,
	title = {Face {Mask} {Detection} {\textasciitilde}{12K} {Images} {Dataset}},
	url = {https://www.kaggle.com/datasets/ashishjangra27/face-mask-12k-images-dataset},
	abstract = {12K Images divided in training testing and validation directories.},
	language = {en},
	urldate = {2023-05-01},
	file = {Snapshot:H\:\\Zotero\\storage\\2C2PIDFB\\face-mask-12k-images-dataset.html:text/html},
}

@misc{kaiserDepthwiseSeparableConvolutions2017,
	title = {Depthwise {Separable} {Convolutions} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1706.03059},
	doi = {10.48550/arXiv.1706.03059},
	abstract = {Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new "super-separable" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Kaiser, Lukasz and Gomez, Aidan N. and Chollet, Francois},
	month = jun,
	year = {2017},
	note = {arXiv:1706.03059 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\85E8NUG5\\Kaiser et al. - 2017 - Depthwise Separable Convolutions for Neural Machin.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\LDY46U6U\\1706.html:text/html},
}

@misc{MachineLearningCollectionMLPytorch,
	title = {Machine-{Learning}-{Collection}/{ML}/{Pytorch}/object\_detection/{YOLOv3} at master · aladdinpersson/{Machine}-{Learning}-{Collection}},
	url = {https://github.com/aladdinpersson/Machine-Learning-Collection},
	abstract = {A resource for learning about Machine learning \& Deep Learning - Machine-Learning-Collection/ML/Pytorch/object\_detection/YOLOv3 at master · aladdinpersson/Machine-Learning-Collection},
	language = {en},
	urldate = {2023-05-01},
	journal = {GitHub},
	note = {Retrieved May 1, 2023 from https://github.com/aladdinpersson/Machine-Learning-Collection},
	file = {Snapshot:H\:\\Zotero\\storage\\HPYLDXHB\\YOLOv3.html:text/html},
}

@misc{mahajanExploringLimitsWeakly2018,
	title = {Exploring the {Limits} of {Weakly} {Supervised} {Pretraining}},
	url = {http://arxiv.org/abs/1805.00932},
	doi = {10.48550/arXiv.1805.00932},
	abstract = {State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4\% (97.6\% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
	month = may,
	year = {2018},
	note = {arXiv:1805.00932 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Technical report},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\M3SQRMQF\\Mahajan et al. - 2018 - Exploring the Limits of Weakly Supervised Pretrain.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\NNIP586Z\\1805.html:text/html},
}

@misc{sarmientoPavementDistressDetection2021,
	title = {Pavement {Distress} {Detection} and {Segmentation} using {YOLOv4} and {DeepLabv3} on {Pavements} in the {Philippines}},
	url = {http://arxiv.org/abs/2103.06467},
	doi = {10.48550/arXiv.2103.06467},
	abstract = {Road transport infrastructure is critical for safe, fast, economical, and reliable mobility within the whole country that is conducive to a productive society. However, roads tend to deteriorate over time due to natural causes in the environment and repeated traffic loads. Pavement Distress (PD) detection is essential in monitoring the current conditions of the public roads to enable targeted rehabilitation and preventive maintenance. Nonetheless, distress detection surveys are still done via manual inspection for developing countries such as the Philippines. This study proposed the use of deep learning for two ways of recording pavement distresses from 2D RGB images - detection and segmentation. YOLOv4 is used for pavement distress detection while DeepLabv3 is employed for pavement distress segmentation on a small dataset of pavement images in the Philippines. This study aims to provide a basis to potentially spark solutions in building a cheap, scalable, and automated end-to-end solution for PD detection in the country.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Sarmiento, James-Andrew},
	month = mar,
	year = {2021},
	note = {arXiv:2103.06467 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.0},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\WCKAUBLN\\Sarmiento - 2021 - Pavement Distress Detection and Segmentation using.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\55B9SSLY\\2103.html:text/html;Full Text:H\:\\Zotero\\storage\\SKHN5N4V\\Sarmiento - 2021 - Pavement Distress Detection and Segmentation using.pdf:application/pdf},
}

@inproceedings{adarshYOLOV3TinyObject2020,
	title = {{YOLO} v3-{Tiny}: {Object} {Detection} and {Recognition} using one stage improved model},
	shorttitle = {{YOLO} v3-{Tiny}},
	doi = {10.1109/ICACCS48705.2020.9074315},
	abstract = {Object detection has seen many changes in algorithms to improve performance both on speed and accuracy. By the continuous effort of so many researchers, deep learning algorithms are growing rapidly with an improved object detection performance. Various popular applications like pedestrian detection, medical imaging, robotics, self-driving cars, face detection, etc. reduces the efforts of humans in many areas. Due to the vast field and various state-of-the-art algorithms, it is a tedious task to cover all at once. This paper presents the fundamental overview of object detection methods by including two classes of object detectors. In two stage detector covered algorithms are RCNN, Fast RCNN, and Faster RCNN, whereas in one stage detector YOLO v1, v2, v3, and SSD are covered. Two stage detectors focus more on accuracy, whereas the primary concern of one stage detectors is speed. We will explain an improved YOLO version called YOLO v3-Tiny, and then its comparison with previous methods for detection and recognition of object is described graphically.},
	booktitle = {2020 6th {International} {Conference} on {Advanced} {Computing} and {Communication} {Systems} ({ICACCS})},
	author = {Adarsh, Pranav and Rathi, Pratibha and Kumar, Manoj},
	month = mar,
	year = {2020},
	note = {ISSN: 2575-7288},
	keywords = {Computational modeling, Deep learning, Detectors, Machine learning, Object detection, YOLO v3, Convolutional Neural Networks, Proposals, Object recognition, Computer vision, Communication systems, Faster RCNN, image processing, YOLO v3-Tiny},
	pages = {687--694},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\NS3XWT77\\9074315.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\HWR2DCWK\\Adarsh et al. - 2020 - YOLO v3-Tiny Object Detection and Recognition usi.pdf:application/pdf},
}

@article{fangTinierYOLORealTimeObject2020,
	title = {Tinier-{YOLO}: {A} {Real}-{Time} {Object} {Detection} {Method} for {Constrained} {Environments}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Tinier-{YOLO}},
	doi = {10.1109/ACCESS.2019.2961959},
	abstract = {Deep neural networks (DNNs) have shown prominent performance in the field of object detection. However, DNNs usually run on powerful devices with high computational ability and sufficient memory, which have greatly limited their deployment for constrained environments such as embedded devices. YOLO is one of the state-of-the-art DNN-based object detection approaches with good performance both on speed and accuracy and Tiny-YOLO-V3 is its latest variant with a small model that can run on embedded devices. In this paper, Tinier-YOLO, which is originated from Tiny-YOLO-V3, is proposed to further shrink the model size while achieving improved detection accuracy and real-time performance. In Tinier-YOLO, the fire module in SqueezeNet is appointed by investigating the number of fire modules as well as their positions in the model in order to reduce the number of model parameters and then reduce the model size. For further improving the proposed Tinier-YOLO in terms of detection accuracy and real-time performance, the connectivity style between fire modules in Tinier-YOLO differs from SqueezeNet in that dense connection is introduced and fine designed to strengthen the feature propagation and ensure the maximum information flow in the network. The object detection performance is enhanced in Tinier-YOLO by using the passthrough layer that merges feature maps from the front layers to get fine-grained features, which can counter the negative effect of reducing the model size. The resulting Tinier-YOLO yields a model size of 8.9MB (almost 4× smaller than Tiny-YOLO-V3) while achieving 25 FPS real-time performance on Jetson TX1 and an mAP of 65.7\% on PASCAL VOC and 34.0\% on COCO. Tinier-YOLO alse posses comparable results in mAP and faster runtime speed with smaller model size and BFLOP/s value compared with other lightweight models like SqueezeNet SSD and MobileNet SSD.},
	journal = {IEEE Access},
	author = {Fang, Wei and Wang, Lin and Ren, Peiming},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Computational modeling, Detectors, Object detection, Convolution, Real-time systems, YOLO, Performance evaluation, Constrained environments, dense connection, fire modules, passthrough layer},
	pages = {1935--1944},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\6CSXKK8H\\8941141.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\CEZ9YNZM\\Fang et al. - 2020 - Tinier-YOLO A Real-Time Object Detection Method f.pdf:application/pdf},
}

@inproceedings{lanPedestrianDetectionBased2018,
	title = {Pedestrian {Detection} {Based} on {YOLO} {Network} {Model}},
	doi = {10.1109/ICMA.2018.8484698},
	abstract = {After going through the deep network, there will be some loss of pedestrian information, which will cause the disappearance of gradients, causing inaccurate pedestrian detection. This paper improves the network structure of YOLO algorithm and proposes a new network structure YOLO-R. First, three Passthrough layers were added to the original YOLO network. The Passthrough layer consists of the Route layer and the Reorg layer. Its role is to connect the shallow layer pedestrian features to the deep layer pedestrian features and link the high and low resolution pedestrian features. The role of the Route layer is to pass the pedestrian characteristic information of the specified layer to the current layer, and then use the Reorg layer to reorganize the feature map so that the currently-introduced Route layer feature can be matched with the feature map of the next layer. The three Passthrough layers added in this algorithm can well transfer the network's shallow pedestrian fine-grained features to the deep network, enabling the network to better learn shallow pedestrian feature information. This paper also changes the layer number of the Passthrough layer connection in the original YOLO algorithm from Layer 16 to Layer 12 to increase the ability of the network to extract the information of the shallow pedestrian features. The improvement was tested on the INRIA pedestrian dataset. The experimental results show that this method can effectively improve the detection accuracy of pedestrians, while reducing the false detection rate and the missed detection rate, and the detection speed can reach 25 frames per second.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Mechatronics} and {Automation} ({ICMA})},
	author = {Lan, Wenbo and Dang, Jianwu and Wang, Yangping and Wang, Song},
	month = aug,
	year = {2018},
	note = {ISSN: 2152-744X},
	keywords = {Feature extraction, Training, Machine learning, Data models, Pedestrian Detection, Data mining, Predictive models, Deformable models, Passthrough Layer, Reorg Layer, Route Layer, YOLO Network Model},
	pages = {1547--1551},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\Y6BGMG2L\\8484698.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\J3ZTHTRH\\Lan et al. - 2018 - Pedestrian Detection Based on YOLO Network Model.pdf:application/pdf},
}

@misc{redmonYouOnlyLook2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	doi = {10.48550/arXiv.1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv:1506.02640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\7XP4GHHP\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\P28EV6S5\\1506.html:text/html},
}

@misc{redmonYOLO9000BetterFaster2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	doi = {10.48550/arXiv.1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv:1612.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\WNAUXZC4\\Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\3MYI8S4C\\1612.html:text/html},
}

@misc{tervenComprehensiveReviewYOLO2023,
	title = {A {Comprehensive} {Review} of {YOLO}: {From} {YOLOv1} to {YOLOv8} and {Beyond}},
	shorttitle = {A {Comprehensive} {Review} of {YOLO}},
	url = {http://arxiv.org/abs/2304.00501},
	abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO to YOLOv8. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
	urldate = {2023-05-09},
	publisher = {arXiv},
	author = {Terven, Juan and Cordova-Esparza, Diana},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00501 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 27 pages, 12 figures, 4 tables, submitted to ACM Computing Surveys},
	file = {arXiv.org Snapshot:H\:\\Zotero\\storage\\7HFEXF6E\\2304.html:text/html;Full Text PDF:H\:\\Zotero\\storage\\D72M89VZ\\Terven and Cordova-Esparza - 2023 - A Comprehensive Review of YOLO From YOLOv1 to YOL.pdf:application/pdf},
}

@misc{jocherYOLOUltralytics2023,
	title = {{YOLO} by {Ultralytics}},
	copyright = {AGPL-3.0},
	url = {https://github.com/ultralytics/ultralytics},
	abstract = {NEW - YOLOv8 🚀 in PyTorch {\textgreater} ONNX {\textgreater} CoreML {\textgreater} TFLite},
	urldate = {2023-05-09},
	author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
	month = jan,
	year = {2023},
	note = {original-date: 2022-09-11T16:39:45Z},
}

@inproceedings{liDesignAutomaticRecycling2022,
	title = {Design of {Automatic} {Recycling} {Robot} {Based} on {YOLO} {Target} {Detection}},
	doi = {10.1109/CACML55074.2022.00059},
	abstract = {In order to achieve automatic item grasping and recovery, we propose a system design method based on YOLO v4 automatic recovery robot, using the higher computing power of Jetson Nano and STM32F103ZET6 computing units, processing image information to control the operation of the robot system, with six degrees of freedom PWM robot arm to accurately grasp the items. After system testing, the average item recognition rate exceeds 98.5\%, and the recovery success rate exceeds 96\%, truly achieving automatic search, grasp, recovery, and return end-to-end operation.},
	booktitle = {2022 {Asia} {Conference} on {Algorithms}, {Computing} and {Machine} {Learning} ({CACML})},
	author = {Li, Chuan and Shu, Manming and Du, Ling and Tan, Haoyue and Wei, Lang},
	month = mar,
	year = {2022},
	keywords = {Object detection, Machine learning algorithms, item recovery, Manipulators, mechanical control, Process control, Pulse width modulation, PWM control, Shape, System testing, Yolo target detection},
	pages = {310--315},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\A9NK53J4\\9852493.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\7RNHSB6G\\Li et al. - 2022 - Design of Automatic Recycling Robot Based on YOLO .pdf:application/pdf},
}

@misc{sagarYOLOCreatorQuits2020,
	title = {{YOLO} {Creator} {Quits} {AI} {Research} {Citing} {Ethical} {Concerns}},
	url = {https://analyticsindiamag.com/yolo-creator-joe-redmon-computer-vision-research-ethical-concern/},
	abstract = {Joe Redmon, creator of the popular YOLO computer vision algorithm has said in his recent Twitter post that he has quit research},
	language = {en-US},
	urldate = {2023-05-10},
	journal = {Analytics India Magazine},
	author = {Sagar, Ram},
	month = feb,
	year = {2020},
	file = {Snapshot:H\:\\Zotero\\storage\\GT849235\\yolo-creator-joe-redmon-computer-vision-research-ethical-concern.html:text/html},
}

@misc{YOLORealTimeObject,
	title = {{YOLO}: {Real}-{Time} {Object} {Detection}},
	url = {https://pjreddie.com/darknet/yolo/},
	urldate = {2023-05-10},
	file = {YOLO\: Real-Time Object Detection:H\:\\Zotero\\storage\\NHYTMWPB\\yolo.html:text/html},
}

@phdthesis{maoMemoryEfficientYOLOObject2021,
	title = {A {Memory}-{Efficient} {YOLO} {Object} {Detection} {Convolutional} {Neural} {Network} {Inference} {Engine} {On} {The} {KiloCore} 2 {Manycore} {Platform}},
	url = {https://escholarship.org/uc/item/1b5393kk},
	abstract = {Object Detection is one of the most resource-intensive tasks for Convolutional Neural Networks (CNN). To predict the category of the objects and at the same time determine their location, the Object Detection network has to use a very deep structure, typically 10 to 50 layers, along with a huge number of learnable parameters ranging from a few million to over a billion. Many architectures have been implemented on various hardware platforms to accelerate the inference speed of Object Detection. For example, the cuDNN library on Nvidia GPUs and the Intel DLIA framework on x86 CPUs. However, they either consume a lot of power or require large memory to perform the acceleration algorithm, making them unsuitable for edge-computing use cases where power is limited and energy must be conserved.This thesis presents an efficient and high-throughput network inference implementation for the YOLOv3-Tiny Object Detection system on the KiloCore 2 manycore platform. YOLOv3-Tiny is a light-weight and accurate Object Detection network with only 13 Convolution layers and 8,861,918 learnable parameters, and the KiloCore 2 platform is a low-power manycore processor chip with 697 programmable cores and a high-speed on-chip communication network.Specifically, this thesis presents two software optimization techniques to relax the memory requirements of YOLOv3-Tiny, and a scalable hardware architecture for calculating convolutions. On the software side, low-precision quantization reduces all the parameters from 16-bits to 8-bits while still maintaining 90\% of the accuracy, and Batch Normalization (BN) Folding is used to compress the computation complexity of the network, removing all the BN layers together with 12,736 parameters. This thesis describes a standardized process of applying quantization and BN Folding so that these optimization techniques can be implemented on any CNN. On the hardware side, a scalable and modular architecture to calculate convolution utilizing a maximum of 536 cores on KiloCore 2 is presented, achieving a high throughput per chip area of 1.002 frames per second/cm{\textasciicircum}2 and offers a low energy consumption of 2.232 J/image.Compared with other hardware platforms such as general-purpose CPUs and specialized GPU accelerators, this implementation achieves a less than 5\% reduction in throughput per chip area, but offers 9.17x to 441x greater throughput per watt. Furthermore, to run the full YOLOv3-Tiny network, this implementation requires only 17.72 MB of off-chip memory for parameters and 896 KB of on-chip memory, which provides a 49.5x to 64.8x memory reduction compared with the GPU implementations.},
	language = {en},
	urldate = {2023-05-10},
	school = {UC Davis},
	author = {Mao, Yikai},
	year = {2021},
	file = {Full Text PDF:H\:\\Zotero\\storage\\MKBAGNZ2\\Mao - 2021 - A Memory-Efficient YOLO Object Detection Convoluti.pdf:application/pdf},
}

@inproceedings{wangEfficientYoloLightweight2020,
	title = {Efficient {Yolo}: {A} {Lightweight} {Model} {For} {Embedded} {Deep} {Learning} {Object} {Detection}},
	shorttitle = {Efficient {Yolo}},
	doi = {10.1109/ICMEW46912.2020.9105997},
	abstract = {It is essential to pursue efficiency for on-road object detection task. To incorporate deep model into embedded devices while maintaining high accuracy, in this paper, an Efficient YOLO framework is rebuilt based on traditional YOLOv3. Firstly, an iterative initialization strategy is designed to ensure the network sparsity in the initial training. Then comprehensive pruning schemes including layer-level and channel-wise pruning are proposed to lighten the model parameters.With the support of external dataset, the detection accuracy remains at a high level. Compared with the orignal version, our model shrinks the model size by 96.93\% and calculation amount by 84.36\%. The inference speed is improved 2.23 times on NVIDIA Jetson TX2 platform. Finally, we achieve a mAP of 0.492 on the testing dataset, and rank the top accuracy of ICME competition.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Multimedia} \& {Expo} {Workshops} ({ICMEW})},
	author = {Wang, Zixuan and Zhang, Jiacheng and Zhao, Zhicheng and Su, Fei},
	month = jul,
	year = {2020},
	keywords = {Training, Computational modeling, Deep learning, Task analysis, Object detection, Conferences, YOLO, embedded system, model pruning, Testing},
	pages = {1--6},
	annote = {Pruning done on this study!

},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\6XN2ET9C\\9105997.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\Z7IDDWER\\Wang et al. - 2020 - Efficient Yolo A Lightweight Model For Embedded D.pdf:application/pdf},
}

@inproceedings{dingREQYOLOResourceAwareEfficient2019,
	address = {New York, NY, USA},
	series = {{FPGA} '19},
	title = {{REQ}-{YOLO}: {A} {Resource}-{Aware}, {Efficient} {Quantization} {Framework} for {Object} {Detection} on {FPGAs}},
	isbn = {978-1-4503-6137-8},
	shorttitle = {{REQ}-{YOLO}},
	url = {https://dl.acm.org/doi/10.1145/3289602.3293904},
	doi = {10.1145/3289602.3293904},
	abstract = {Deep neural networks (DNNs), as the basis of object detection, will play a key role in the development of future autonomous systems with full autonomy. The autonomous systems have special requirements of real-time, energy-e cient implementations of DNNs on a power-budgeted system. Two research thrusts are dedicated to per- formance and energy e ciency enhancement of the inference phase of DNNs. The first one is model compression techniques while the second is e cient hardware implementations. Recent researches on extremely-low-bit CNNs such as binary neural network (BNN) and XNOR-Net replace the traditional oating point operations with bi- nary bit operations, signi cantly reducing memory bandwidth and storage requirement, whereas suffering non-negligible accuracy loss and waste of digital signal processing (DSP) blocks on FPGAs. To overcome these limitations, this paper proposes REQ-YOLO, a resource aware, systematic weight quantization framework for object detection, considering both algorithm and hardware resource aspects in object detection. We adopt the block-circulant matrix method and propose a heterogeneous weight quantization using Alternative Direction Method of Multipliers (ADMM), an e ective optimization technique for general, non-convex optimization problems. To achieve real-time, highly efficient implementations on FPGA, we present the detailed hardware implementation of block circulant matrices on CONV layers and de- velop an e cient processing element (PE) structure supporting the heterogeneous weight quantization, CONV data ow and pipelining techniques, design optimization, and a template-based automatic synthesis framework to optimally exploit hardware resource. Experimental results show that our proposed REQ-YOLO framework can signi cantly compress the YOLO model while introducing very small accuracy degradation. The related codes are here: https://github.com/Anonymous788/heterogeneous\_ADMM\_YOLO.},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 2019 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Caiwen and Wang, Shuo and Liu, Ning and Xu, Kaidi and Wang, Yanzhi and Liang, Yun},
	month = feb,
	year = {2019},
	keywords = {object detection, admm, compression, fpga, yolo},
	pages = {33--42},
	annote = {Quantization done on this study!
},
	file = {Full Text PDF:H\:\\Zotero\\storage\\H72RHQDV\\Ding et al. - 2019 - REQ-YOLO A Resource-Aware, Efficient Quantization.pdf:application/pdf},
}

@article{hanSMDYOLOEfficientLightweight2022,
	title = {{SMD}-{YOLO}: {An} efficient and lightweight detection method for mask wearing status during the {COVID}-19 pandemic},
	volume = {221},
	issn = {0169-2607},
	shorttitle = {{SMD}-{YOLO}},
	url = {https://www.sciencedirect.com/science/article/pii/S016926072200270X},
	doi = {10.1016/j.cmpb.2022.106888},
	abstract = {Background and Objective
At present, the COVID-19 epidemic is still spreading worldwide and wearing a mask in public areas is an effective way to prevent the spread of the respiratory virus. Although there are many deep learning methods used for detecting the face masks, there are few lightweight detectors having a good effect on small or medium-size face masks detection in the complicated environments.
Methods
In this work we propose an efficient and lightweight detection method based on YOLOv4-tiny, and a face mask detection and monitoring system for mask wearing status. Two feasible improvement strategies, network structure optimization and K-means++ clustering algorithm, are utilized for improving the detection accuracy on the premise of ensuring the real-time face masks recognition. Particularly, the improved residual module and cross fusion module are designed to aim at extracting the features of small or medium-size targets effectively. Moreover, the enhanced dual attention mechanism and the improved spatial pyramid pooling module are employed for merging sufficiently the deep and shallow semantic information and expanding the receptive field. Afterwards, the detection accuracy is compensated through the combination of activation functions. Finally, the depthwise separable convolution module is used to reduce the quantity of parameters and improve the detection efficiency. Our proposed detector is evaluated on a public face mask dataset, and an ablation experiment is also provided to verify the effectiveness of our proposed model, which is compared with the state-of-the-art (SOTA) models as well.
Results
Our proposed detector increases the AP (average precision) values in each category of the public face mask dataset compared with the original YOLOv4-tiny. The mAP (mean average precision) is improved by 4.56\% and the speed reaches 92.81 FPS. Meanwhile, the quantity of parameters and the FLOPs (floating-point operations) are reduced by 1/3, 16.48\%, respectively.
Conclusions
The proposed detector achieves better overall detection performance compared with other SOTA detectors for real-time mask detection, demonstrated the superiority with both theoretical value and practical significance. The developed system also brings greater flexibility to the application of face mask detection in hospitals, campuses, communities, etc.},
	language = {en},
	urldate = {2023-05-10},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Han, Zhenggong and Huang, Haisong and Fan, Qingsong and Li, Yiting and Li, Yuqin and Chen, Xingran},
	month = jun,
	year = {2022},
	keywords = {Object detection, Computer vision, COVID-19, YOLO, Face mask detection},
	pages = {106888},
	annote = {Ablation done in this study!
},
	file = {Full Text:H\:\\Zotero\\storage\\734TRAX6\\Han et al. - 2022 - SMD-YOLO An efficient and lightweight detection m.pdf:application/pdf;ScienceDirect Snapshot:H\:\\Zotero\\storage\\QQ4REFPR\\S016926072200270X.html:text/html},
}

@misc{agarwalPedestrianDetectionCount2021,
	title = {Pedestrian detection and count for certain number of frames in a video},
	url = {https://medium.com/analytics-vidhya/pedestrian-detection-and-count-for-certain-number-of-frames-in-a-video-f6563309ef78},
	abstract = {Contents:},
	language = {en},
	urldate = {2023-05-10},
	journal = {Analytics Vidhya},
	author = {Agarwal, Raghav},
	month = mar,
	year = {2021},
	note = {Retrieved May 10, 2023 from https://medium.com/analytics-vidhya/pedestrian-detection-and-count-for-certain-number-of-frames-in-a-video-f6563309ef78},
	file = {Snapshot:H\:\\Zotero\\storage\\8HJMF7I9\\pedestrian-detection-and-count-for-certain-number-of-frames-in-a-video-f6563309ef78.html:text/html},
}

@inproceedings{zhangPrunedYOLOLearningEfficient2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pruned-{YOLO}: {Learning} {Efficient} {Object} {Detector} {Using} {Model} {Pruning}},
	isbn = {978-3-030-86380-7},
	shorttitle = {Pruned-{YOLO}},
	doi = {10.1007/978-3-030-86380-7_4},
	abstract = {Accurate real-time object detection plays a key role in various practical scenarios such as automatic driving and UAV surveillance. The memory limitation and poor computing power of edge devices hinder the deployment of high performance Convolutional Neural Networks (CNNs). Iterative channel pruning is an effective method to obtain lightweight networks. However, the channel importance measurement and iterative pruning in the existing methods are suboptimal. In this paper, to measure the channel importance, we simultaneously consider the scale factor of batch normalization (BN) and the kernel weight of convolutional layers. Besides, sparsity training and fine tuning are combined to simplify the pruning pipeline. Notably, the cosine decay of sparsity coefficient and soft mask strategy are used to optimize our compact model, i.e., Pruned-YOLOv3/v5, which is constructed via pruning YOLOv3/v5. The experimental results on the MS-COCO and VisDrone datasets show that the proposed model achieves a satisfactory balance between computational efficiency and detection accuracy.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2021},
	publisher = {Springer International Publishing},
	author = {Zhang, Jiacheng and Wang, Pingyu and Zhao, Zhicheng and Su, Fei},
	editor = {Farkaš, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
	year = {2021},
	keywords = {Object detection, YOLO, Channel pruning, Model compression},
	pages = {34--45},
}

@inproceedings{liberatoriYOLOBasedFaceMask2022,
	title = {{YOLO}-{Based} {Face} {Mask} {Detection} on {Low}-{End} {Devices} {Using} {Pruning} and {Quantization}},
	doi = {10.23919/MIPRO55190.2022.9803406},
	abstract = {Deploying Deep Learning (DL) based object detection (OD) models in low-end devices, such as single board computers, may lead to poor performance in terms of frames-per-second (FPS). Pruning and quantization are well-known compression techniques that can potentially lead to a reduction of the computational burden of a DL model, with a possible decrease of performance in terms of detection accuracy. Motivated by the widespread introduction of face mask mandates by many institutions during the Covid-19 pandemic, we aim at training and compressing an OD model based on YOLOv4 to recognize the presence of face masks, to be deployed on a Raspberry Pi 4. We investigate the capability of different kinds of pruning and quantization techniques of increasing the FPS with respect to the uncompressed model, while retaining the detection accuracy. We quantitatively assess the pruned and quantized models in terms of Mean Average Precision (mAP) and FPS, and show that with proper pruning and quantization, the FPS can be doubled with a moderate loss in mAP. The results provide guidelines for compression of other OD models based on YOLO.},
	booktitle = {2022 45th {Jubilee} {International} {Convention} on {Information}, {Communication} and {Electronic} {Technology} ({MIPRO})},
	author = {Liberatori, Benedetta and Mami, Ciro Antonio and Santacatterina, Giovanni and Zullich, Marco and Pellegrino, Felice Andrea},
	month = may,
	year = {2022},
	note = {ISSN: 2623-8764},
	keywords = {Training, Computational modeling, Deep learning, Deep Learning, Face recognition, Pandemics, YOLO, Performance evaluation, Face mask detection, Object Detection, Pruning, Quantization, Quantization (signal), TinyML},
	pages = {900--905},
	annote = {Pruning (and explaining pruning) is done here!

},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\PI6PFW8U\\9803406.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\HK5Z4JTH\\Liberatori et al. - 2022 - YOLO-Based Face Mask Detection on Low-End Devices .pdf:application/pdf},
}

@misc{liPruningFiltersEfficient2017,
	title = {Pruning {Filters} for {Efficient} {ConvNets}},
	url = {http://arxiv.org/abs/1608.08710},
	doi = {10.48550/arXiv.1608.08710},
	abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
	urldate = {2023-05-11},
	publisher = {arXiv},
	author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
	month = mar,
	year = {2017},
	note = {arXiv:1608.08710 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	annote = {Godfather of pruning!
},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\TKTZIX7C\\Li et al. - 2017 - Pruning Filters for Efficient ConvNets.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\DXLF6VE2\\1608.html:text/html},
}

@inproceedings{ghoshDeepNetworkPruning2019,
	title = {Deep {Network} {Pruning} for {Object} {Detection}},
	doi = {10.1109/ICIP.2019.8803505},
	abstract = {With the increasing success of deep learning in various applications, there is an increasing need to have deep models that can be used for deployment in real-time and/or resource constrained scenarios. In this context, this paper analyzes the pruning of deep models for object detection in order to reduce the number of weights and hence the number of computations. Very deep networks based on ResNet like architectures, like YOLOv3 have unique challenges when attempting to prune them. This paper proposes a network pruning technique based on agglomerative clustering for the feature extractor and using mutual information for the detector. The performance of the proposed techniques is also compared with that of a relatively shallow network, i.e., YOLOv2. A compression percentage of around 30\% results in a 10\% drop of mean average precision (mAP) in YOLOv3, whereas in YOLOv2 the drop was around 6\% on the COCO dataset.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Ghosh, Sanjukta and Srinivasa, Shashi K K and Amon, Peter and Hutter, Andreas and Kaup, André},
	month = sep,
	year = {2019},
	note = {ISSN: 2381-8549},
	keywords = {Feature extraction, Computational modeling, Deep Learning, Object detection, Biological system modeling, Computer architecture, Correlation, Object Detection, CNN, Clustering, Mutual information, Network Pruning},
	pages = {3915--3919},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\P84W8CI3\\8803505.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\BY7JIVBP\\Ghosh et al. - 2019 - Deep Network Pruning for Object Detection.pdf:application/pdf},
}

@inproceedings{ohn-barCanAppearancePatterns2015,
	title = {Can appearance patterns improve pedestrian detection?},
	doi = {10.1109/IVS.2015.7225784},
	abstract = {This paper studies the usefulness of appearance patterns for the challenging task of pedestrian detection. Despite appearance specific models being common in rigid object detection, the technique is still little understood for pedestrians. Three main approaches for reasoning over orientation, occlusion, and visual cues in obtaining the appearance patterns are compared. This work demonstrates that large gains in detection performance (up to 17 AP points on the challenging KITTI dataset) can be made using a state-of-the-art pedestrian detector.},
	booktitle = {2015 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Ohn-Bar, Eshed and Trivedi, Mohan M.},
	month = jun,
	year = {2015},
	note = {ISSN: 1931-0587},
	keywords = {Feature extraction, Image color analysis, Training, Computational modeling, Detectors, Object detection, Visualization},
	pages = {808--813},
	file = {IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\D88WTALG\\Ohn-Bar and Trivedi - 2015 - Can appearance patterns improve pedestrian detecti.pdf:application/pdf},
}

@inproceedings{chenThermalBasedPedestrianDetection2019,
	title = {Thermal-{Based} {Pedestrian} {Detection} {Using} {Faster} {R}-{CNN} and {Region} {Decomposition} {Branch}},
	doi = {10.1109/ISPACS48206.2019.8986298},
	abstract = {In this paper, we present an infrared thermal-based pedestrian detection method that can be applied in nighttime intelligent surveillance systems. Pedestrian detection plays an important role in computer vision and automation industry applications, which include video surveillance, automotive robot, and smart vehicles. Recently, the improvement in deep learning techniques, such as convolutional neural networks (CNNs), have significantly increased the accuracy of pedestrian detection. Normally, the optical cameras, e.g. charge-coupled device cameras, are the device used to capture images. However, considering the dark environments and the luminance variation issues, infrared thermal camera would be an effective alternative solution to nighttime pedestrian detection. On the other hand, occlusion is one of the commonest problems, which makes nighttime pedestrian detection more challenging. To address the abovementioned problems, this work presents a pedestrian detection framework which consists of Faster R-CNN and a region decomposition branch. The proposed region decomposition branch allows us to detect wider range of the pedestrian appearances including partial body poses and occlusions. From the experimental results, this work demonstrates better detection accuracy than the currently developed CNN-based detection method because of combining the multi-region features.},
	booktitle = {2019 {International} {Symposium} on {Intelligent} {Signal} {Processing} and {Communication} {Systems} ({ISPACS})},
	author = {Chen, Yung-Yao and Jhong, Sin-Ye and Li, Guan-Yi and Chen, Ping-Han},
	month = dec,
	year = {2019},
	note = {ISSN: 2642-3529},
	keywords = {infrared thermal imaging, pedestrian detection, region decomposition branch},
	pages = {1--2},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\9LVHEIV2\\8986298.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\D9JXPV59\\Chen et al. - 2019 - Thermal-Based Pedestrian Detection Using Faster R-.pdf:application/pdf},
}

@inproceedings{huangBetterPedestrianDetection2022,
	title = {Towards {Better} {Pedestrian} {Detection} {Using} {Multi}-{Scale} {CSPN} and {Dual} {Attention}},
	doi = {10.1109/ICTech55460.2022.00096},
	abstract = {Pedestrian detection is a significant research direction in the computer vision, but the detection performance of existing pedestrian detection algorithms is inadequate. Therefore, this article proposes a novel algorithm to improve the anchor-free pedestrian detection algorithm. First, the multi-scale CSPN module is used to deepen the network depth, further extract semantic information on multiple scales, and improve detection performance. Moreover, the dual attention module based on feature fusion is used to effectively fuse features of different scales, assigning new weights to the fused features in the two dimensions of space and channel. Experiments show our method reduces MR−2 by 0.10\%, 2.60\% and 0.98\% on the Reasonable, Heavy Occlusion and ALL of the Caltech pedestrian dataset, which is better than the existing algorithms.},
	booktitle = {2022 11th {International} {Conference} of {Information} and {Communication} {Technology} ({ICTech}))},
	author = {Huang, XinXin and Yin, ZhenYu and Fan, Chao},
	month = feb,
	year = {2022},
	keywords = {Feature extraction, Robustness, Data mining, Semantics, pedestrian detection, Anchor-free, CSP, dual attention, Feature detection, Fuses, Information and communication technology, multi-SCALE},
	pages = {451--456},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\5F6A5ANS\\9849529.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\V6PCNREG\\Huang et al. - 2022 - Towards Better Pedestrian Detection Using Multi-Sc.pdf:application/pdf},
}

@inproceedings{qianEfficientModelCompression2018,
	title = {An {Efficient} {Model} {Compression} {Method} for {CNN} {Based} {Object} {Detection}},
	doi = {10.1109/ICSESS.2018.8663809},
	abstract = {Object detection algorithms like Faster R-CNN and YOLO have demonstrated excellent results on datasets such as the PASCAL VOC and COCO. However, these algorithms need powerful GPUs which makes them infeasible in embedded system because of the huge number of FLOPS and parameters. This paper uses two techniques to solve this problem: depthwise separable convolution and filter pruning. Depthwise separable convolution has been proven to be an efficient technique for reducing FLOPS and parameters in some object detection networks. Filter pruning can also reduce FLOPS and parameters significantly while regaining close to the original accuracy in image classification tasks. This paper also optimizes the pruning method which can greatly reduce pruning time by changing step size according to the sensitivity of each layer. In this work, YOLOv3-tiny network will be modified by depthwise separable convolutions and filter pruning. The results show that FLOPS drops 54\%, parameters and model size drop 71 \% while mAP just decreases 1.85\%.},
	booktitle = {2018 {IEEE} 9th {International} {Conference} on {Software} {Engineering} and {Service} {Science} ({ICSESS})},
	author = {Qian, Liuchen and Fu, Yuzhuo and Liu, Ting},
	month = nov,
	year = {2018},
	note = {ISSN: 2327-0594},
	keywords = {Computational modeling, Deep learning, Object detection, Convolution, depthwise separable convolution, Quantization (signal), model compression, pruning, Sensitivity, Taylor series},
	pages = {766--769},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\N2PETPZZ\\8663809.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\TXJKVJWD\\Qian et al. - 2018 - An Efficient Model Compression Method for CNN Base.pdf:application/pdf},
}

@inproceedings{endrawatiYOLOv3TinyWeight2021,
	title = {{YOLOv3}- {Tiny}'s {Weight} {Size} {Reduction} using {Pruning} and {Quantization}},
	doi = {10.1109/TSSA52866.2021.9768258},
	abstract = {CNN has been widely applied to various technology fields, like real-time edge devices. One of the architecture detection objects based on CNN is YOLO, a real-time detection object with a high level of accuracy. Detection objects on the edge device itself are expected to have speed, accuracy, and less memory needed to carry out the detection process. In this paper, YOLOv3- Tiny will be pruned and quantized to reduce the size of the data from the weight without significant reduction of accuracy. Experimental results show that YOLOv3- Tiny weight data decreased up to 85.7\% compared to the weight data before the pruning process, while in the quantization mode. Moreover, the iterative pruning method can reduce the weight up to 43\%.},
	booktitle = {2021 15th {International} {Conference} on {Telecommunication} {Systems}, {Services}, and {Applications} ({TSSA})},
	author = {Endrawati, Devi Noar and Ibad, Sayyid Irsyadul and Syafalni, Infall and Sutisna, Nana and Mulyawan, Rahmat and Adiono, Trio},
	month = nov,
	year = {2021},
	keywords = {Training, Real-time systems, Image edge detection, Pruning, Quantization, Quantization (signal), Iterative methods, Memory management, Telecommunications, YOLOv3-Tiny},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\GIITI3J5\\9768258.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\TBK44CCN\\Endrawati et al. - 2021 - YOLOv3- Tiny's Weight Size Reduction using Pruning.pdf:application/pdf},
}

@inproceedings{shiOptimizedYolov3Deployment2021,
	title = {Optimized {Yolov3} {Deployment} on {Jetson} {TX2} {With} {Pruning} and {Quantization}},
	doi = {10.1109/ICFTIC54370.2021.9647400},
	abstract = {Pruning and Quantization are commonly techniques deployed on deep convolutional networks to accelerate the model and reducing size, taking the resolution as a sacrifice. In embedded systems where computation power is limited due to power and cost constraints, pruning and quantization is mandatory for such networks especially where real-time processing is required, such as image classification in live video streams. In this paper, a pruned and quantized YOLOv3 model is deployed on Nvidia's industrial standard model Jetson TX2, which demonstrated an increase of 5 FPS in image classification via an 640×480 USB camera, while allocating only 16.7\% storage on disk.},
	booktitle = {2021 {IEEE} 3rd {International} {Conference} on {Frontiers} {Technology} of {Information} and {Computer} ({ICFTIC})},
	author = {Shi, Zhuoxuan},
	month = nov,
	year = {2021},
	keywords = {Image resolution, Computational modeling, Real-time systems, Quantization (signal), pruning, embedded systems, Embedded systems, python, quantization, Streaming media, Universal Serial Bus},
	pages = {62--65},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\PH36KNYZ\\9647400.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\7KWGHU4U\\Shi - 2021 - Optimized Yolov3 Deployment on Jetson TX2 With Pru.pdf:application/pdf},
}

@inproceedings{wuModelCompressionBased2021,
	title = {Model {Compression} {Based} on {YOLOv3} {Object} {Detection} {Algorithm} from the {Perspective} of {UAV}},
	doi = {10.23919/CCC52363.2021.9550707},
	abstract = {Recently, unmanned aerial vehicles are widely used in surveillance, aerial photography, power grid line inspections and other places. In order to deploy the YOLOv3 [1] algorithm on drones, it is necessary to adopt the YOLOv3 algorithm with fewer parameters and a simpler structure. This paper implements the model compression of YOLOv3 based on methods such as sparseness, pruning, and knowledge distillation. This paper implements the sparseness of the network by adding L1 regular expressions on the convolutional layer. After that, redundant channels and layers are removed through channel pruning and layer pruning. After sparse and pruning, the mAP lost a lot. By using knowledge distillation after pruning, it attempts to recover mAP lost in sparseness and pruning. By this method, the YOLOv3 algorithm can be deployed on embedded platforms such as RK3399pro. We evaluate the model on the visdrone2019 dataset. The experimental results show that after model compression, YOLOv3 is more suitable for deployment on embedded platforms.},
	booktitle = {2021 40th {Chinese} {Control} {Conference} ({CCC})},
	author = {Wu, Bing and Xue, Zhijun and Chen, Wenjie},
	month = jul,
	year = {2021},
	note = {ISSN: 1934-1768},
	keywords = {Object detection, Surveillance, YOLOv3, Object Detection, Drones, Inspection, Model Compress, Photography, Power grids, RK3399pro, Simulation},
	pages = {8439--8444},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\UE2Y5KRB\\9550707.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\R53AVSQL\\Wu et al. - 2021 - Model Compression Based on YOLOv3 Object Detection.pdf:application/pdf},
}

@inproceedings{sunYoloBasedLightweightObject2023,
	title = {Yolo-{Based} {Lightweight} {Object} {Detection} {With} {Structure} {Simplification} {And} {Attention} {Enhancement}},
	doi = {10.1109/ICASSP49357.2023.10097155},
	abstract = {In this paper we propose a lightweight object detector by optimizing the structure of YOLOv3. The optimization is conducted in two aspects: simplifying the structural components by lightweight substitutes and introducing the attention mechanism to increase the detection accuracy. For the simplification, we remodel the backbone based on MobileNet v2 and replace every 3×3 convolution in the detection neck and head by the fusion of a 3 × 3 depthwise separable convolution and a squeeze and excitation block (DSConv+SE); for the attention enhancement, we introduce the high-frequency wavelets of the original image to the input, a simplified non- local block to the simplified backbone and convolutional block attention modules to the simplified detection neck. In addition, local 3×3 convolution branches are introduced to the simplified backbone for enhanced learning capability. Experiments demonstrate that the proposed detector outperforms each compared state-of-the-art work in one or more aspects.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Sun, Shuqi and Yang, Xiaohui and Peng, Jingliang},
	month = jun,
	year = {2023},
	keywords = {convolutional neural network, Detectors, Object detection, Benchmark testing, Convolution, Neural networks, YOLO, Head, Lightweight object detection, Signal processing algorithms},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\WIBCIEV3\\10097155.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\9DI85LID\\Sun et al. - 2023 - Yolo-Based Lightweight Object Detection With Struc.pdf:application/pdf},
}

@misc{animatedaiGroupsDepthwiseDepthwiseSeparable2023,
	title = {Groups, {Depthwise}, and {Depthwise}-{Separable} {Convolution} ({Neural} {Networks})},
	url = {https://www.youtube.com/watch?v=vVaRhZXovbw},
	abstract = {Fully animated explanation of the groups option in convolutional neural networks followed by an explanation of depthwise and depthwise-separable convolution in neural networks.

Animations: https://animatedai.github.io/

Intro sound: "Whoosh water x4" by beman87 https://freesound.org/s/162839/},
	urldate = {2023-05-12},
	author = {{Animated AI}},
	month = feb,
	year = {2023},
	note = {Retrieved May 12, 2023 from https://www.youtube.com/watch?v=vVaRhZXovbw},
}

@misc{TensorFlowModelOptimization,
	title = {{TensorFlow} {Model} {Optimization} {Toolkit} — {Pruning} {API}},
	url = {https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html},
	abstract = {The TensorFlow blog contains regular news from the TensorFlow team and the community, with articles on Python, TensorFlow.js, TF Lite, TFX, and more.},
	language = {en},
	urldate = {2023-05-12},
	note = {Retrieved May 12, 2023 from https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html},
	file = {Snapshot:H\:\\Zotero\\storage\\GEEGKHJQ\\tf-model-optimization-toolkit-pruning-API.html:text/html},
}

@misc{ConvolutionBlockAttention2020,
	title = {Convolution {Block} {Attention} {Module} ({CBAM})},
	url = {https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/},
	abstract = {This post covers an in-depth analysis of the Convolution Block Attention Module (CBAM) with a short introduction to Visual Attention Mechanisms.},
	language = {en},
	urldate = {2023-05-12},
	journal = {Paperspace Blog},
	month = jul,
	year = {2020},
	file = {Snapshot:H\:\\Zotero\\storage\\EKUQ6KRV\\attention-mechanisms-in-computer-vision-cbam.html:text/html},
}

@incollection{wooCBAMConvolutionalBlock2018,
	address = {Cham},
	title = {{CBAM}: {Convolutional} {Block} {Attention} {Module}},
	volume = {11211},
	isbn = {978-3-030-01233-5 978-3-030-01234-2},
	shorttitle = {{CBAM}},
	url = {https://link.springer.com/10.1007/978-3-030-01234-2_1},
	abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet eﬀective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature reﬁnement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classiﬁcation and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.},
	language = {en},
	urldate = {2023-05-15},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01234-2_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--19},
	file = {Woo et al. - 2018 - CBAM Convolutional Block Attention Module.pdf:H\:\\Zotero\\storage\\8LAG9HZ9\\Woo et al. - 2018 - CBAM Convolutional Block Attention Module.pdf:application/pdf},
}

@misc{anwarDifferenceAlexNetVGGNet2022,
	title = {Difference between {AlexNet}, {VGGNet}, {ResNet} and {Inception}},
	url = {https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96},
	abstract = {AlexNet, VGGNet, ResNet and Inception explained},
	language = {en},
	urldate = {2023-05-23},
	journal = {Medium},
	author = {Anwar, Aqeel},
	month = jan,
	year = {2022},
	file = {Snapshot:H\:\\Zotero\\storage\\B5V5RBFZ\\the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96.html:text/html},
}

@misc{simonyanVeryDeepConvolutional2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\PXJXCSF2\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\2E9TNBZ6\\1409.html:text/html},
}

@inproceedings{lopezRealtimeFaceMask2021,
	title = {Real-time {Face} {Mask} {Detection} {Using} {Deep} {Learning} on {Embedded} {Systems}},
	doi = {10.1109/ICECIE52348.2021.9664684},
	abstract = {Coronavirus disease (COVID-19) is an infectious disease, which is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that was identified in December 2019 in Wuhan, China [1], [2]. It is a pandemic that causes respiratory disorder and is transmitted through sneezing droplets of infected individuals. These droplets can fall on the objects around the effected and enter a healthy individual through contact. Major symptoms of this disease include lethargy, dry cough, followed by fever [3]. The number of cases is surging dramatically, raping developed and undeveloped countries together [3]. According to the World Health Organization (WHO) COVID-19 weekly epidemiological Update for 29th of December there are 79 million infected cases and 1.7 million deaths globally. This pandemic not only affects our health but also affects our livelihood. In the absence of specific treatment or a vaccine, non-pharmaceutical interventions (NPI) form the backbone of the response to the COVID-19 pandemic. These NPI includes physical distancing, regular hand washing, and wearing a face mask. This study aims to help with the monitoring of these NPIs specifically wearing face masks using deep learning. This study implements face mask detection and recognition system that automatically detects and recognizes if a person is wearing a Medically approved face mask, Non-Medically approved face mask, or not wearing a mask at all. This study has determined that MobileNetV1 model has shown the best performance regarding classification (79\%) and processing speed up to 3.25 fps.},
	booktitle = {2021 3rd {International} {Conference} on {Electrical}, {Control} and {Instrumentation} {Engineering} ({ICECIE})},
	author = {Lopez, Vidal Wyatt M. and Abu, Patricia Angela R. and Estuar, Ma. Regina Justina E.},
	month = nov,
	year = {2021},
	keywords = {convolutional neural network, computer vision, Deep learning, Real-time systems, Neural networks, Organizations, COVID-19, Face recognition, Pandemics, Face mask detection, embedded systems},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\KIDIMJ8F\\metrics.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\SDLEIYKV\\Lopez et al. - 2021 - Real-time Face Mask Detection Using Deep Learning .pdf:application/pdf},
}

@misc{AnomalyIdentificationCritical,
	title = {Anomaly identification of critical power plant facilities based on {YOLOX}-{CBAM}},
	url = {https://ieeexplore-ieee-org.oca.rizal.library.remotexs.co/document/9881135},
	abstract = {Aiming at the abnormal phenomena of steam leakage, oil leakage and water leakage in the key facilities of the power plant, this paper proposes an anomaly detection method based on YOLOX and improves the network and parameters, and the improved network can be better applied to the anomaly detection of power plants. YOLOX is an improved version of the YOLO series, using Anchor-Free, Mosaic, SimOTA and other methods to improve the network. This article adds CBAM (Convolutional Block Attention Module) on this basis and the final experiment shows that the mAP of the YOLOX-CBAM model is 2.7\% higher than that of YOLOX and the detection speed is only 1.4 FPS slower.},
	language = {en-US},
	urldate = {2023-07-09},
	file = {Snapshot:H\:\\Zotero\\storage\\B9MZZWK6\\9881135.html:text/html},
}

@inproceedings{daogangAnomalyIdentificationCritical2022,
	title = {Anomaly identification of critical power plant facilities based on {YOLOX}-{CBAM}},
	doi = {10.1109/PSGEC54663.2022.9881135},
	abstract = {Aiming at the abnormal phenomena of steam leakage, oil leakage and water leakage in the key facilities of the power plant, this paper proposes an anomaly detection method based on YOLOX and improves the network and parameters, and the improved network can be better applied to the anomaly detection of power plants. YOLOX is an improved version of the YOLO series, using Anchor-Free, Mosaic, SimOTA and other methods to improve the network. This article adds CBAM (Convolutional Block Attention Module) on this basis and the final experiment shows that the mAP of the YOLOX-CBAM model is 2.7\% higher than that of YOLOX and the detection speed is only 1.4 FPS slower.},
	booktitle = {2022 {Power} {System} and {Green} {Energy} {Conference} ({PSGEC})},
	author = {Daogang, Peng and Ming, Ge and Danhao, Wang and Jie, Hu},
	month = aug,
	year = {2022},
	keywords = {Anomaly detection, CBAM, exception recognition, Oils, Power generation, Power systems, YOLOX},
	pages = {649--653},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\YAVMCZNI\\9881135.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\X6WH4Q3J\\Daogang et al. - 2022 - Anomaly identification of critical power plant fac.pdf:application/pdf},
}

@inproceedings{guoSARShipDetection2022,
	title = {{SAR} {Ship} {Detection} {Based} on {YOLOv5} {Using} {CBAM} and {BiFPN}},
	doi = {10.1109/IGARSS46834.2022.9884180},
	abstract = {In recent years, deep learning has made breakthroughs in the field of computer vision, the single-stage detection algorithm represented by You Only Look Once (YOLO) has achieved satisfying detection results in SAR ship target detection. For the multi-scale problem of SAR ship targets in complex scenes, we proposed an improved YOLOv5 detection method using Convolutional Block Attention Module (CBAM) and Bidirectional Feature Pyramid Network (BiFPN). The CBAM module and BiFPN are added in YOLOv5 so that it can fully learn the feature information of space and channel dimensions, and enhance information fusion transfer between multi-scale targets. Experiments on our dataset show that the proposed YOLOv5 algorithm achieves 92.8\% Average Precision (AP), which gains a 1.9\% improvement in AP compared to the standard YOLOv5 algorithm in SAR ship target detection. The problem of missed detection of multi-scale targets is well solved.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Guo, Yue and Chen, Shiqi and Zhan, Ronghui and Wang, Wei and Zhang, Jun},
	month = jul,
	year = {2022},
	note = {ISSN: 2153-7003},
	keywords = {Feature extraction, Deep learning, Object detection, Computer vision, CBAM, BiFPN, Geoscience and remote sensing, Marine vehicles, Neck, ship detection, Synthetic Aperture Radar (SAR), YOLOv5},
	pages = {2147--2150},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\PTWDTM3U\\9884180.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\ZBTKWQAD\\Guo et al. - 2022 - SAR Ship Detection Based on YOLOv5 Using CBAM and .pdf:application/pdf},
}

@misc{vuckovicMathematicalTheoryAttention2020,
	title = {A {Mathematical} {Theory} of {Attention}},
	url = {http://arxiv.org/abs/2007.02876},
	abstract = {Attention is a powerful component of modern neural networks across a wide variety of domains. However, despite its ubiquity in machine learning, there is a gap in our understanding of attention from a theoretical point of view. We propose a framework to fill this gap by building a mathematically equivalent model of attention using measure theory. With this model, we are able to interpret self-attention as a system of self-interacting particles, we shed light on self-attention from a maximum entropy perspective, and we show that attention is actually Lipschitz-continuous (with an appropriate metric) under suitable assumptions. We then apply these insights to the problem of mis-specified input data; infinitely-deep, weight-sharing self-attention networks; and more general Lipschitz estimates for a specific type of attention studied in concurrent work.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Vuckovic, James and Baratin, Aristide and Combes, Remi Tachet des},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02876 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:H\:\\Zotero\\storage\\PMMXH6EE\\2007.html:text/html;Full Text PDF:H\:\\Zotero\\storage\\KQC4NXFA\\Vuckovic et al. - 2020 - A Mathematical Theory of Attention.pdf:application/pdf},
}

@inproceedings{parkRobustThermalInfrared2022,
	title = {Robust {Thermal} {Infrared} {Pedestrian} {Detection} {By} {Associating} {Visible} {Pedestrian} {Knowledge}},
	url = {https://ieeexplore.ieee.org/document/9746886},
	doi = {10.1109/ICASSP43922.2022.9746886},
	abstract = {Recently, pedestrian detection on thermal infrared images has shown the robust pedestrian detection performance. In this paper, we propose a novel thermal infrared pedestrian detection framework which can associate and utilize the complementary pedestrian knowledge from visible images. Motivated by that humans can associate useful information from other sensors to perform a more reliable decision, we devise a Visible-sensory Pedestrian Associating (VPA) Memory to conduct the robust pedestrian detection by utilizing complementary visible-sensory pedestrian knowledge explicitly. The VPA Memory is trained to store the pedestrian information of visible images and associate it with a given thermal infrared pedestrian knowledge via the memory associating learning. We verify the effectiveness of the proposed framework with extensive experiments, and it achieves state-of-the-art pedestrian detection performance on thermal infrared images.},
	urldate = {2023-10-19},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Park, Sungjune and Hwi Choi, Dae and Uk Kim, Jung and Ro, Yong Man},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {4468--4472},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\II8ILTDQ\\9746886.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\GTFEWFJH\\Park et al. - 2022 - Robust Thermal Infrared Pedestrian Detection By As.pdf:application/pdf},
}

@inproceedings{liInfraredImagePedestrian2021,
	title = {Infrared {Image} {Pedestrian} {Detection} via {YOLO}-{V3}},
	volume = {5},
	url = {https://ieeexplore.ieee.org/document/9390896},
	doi = {10.1109/IAEAC50856.2021.9390896},
	abstract = {Thermal imaging is often used for pedestrian detection at night to make up for the inability of traditional cameras to be used in harsh lighting conditions. However, the pedestrian detection technology based on thermal imaging is difficult to overcome the interference of high-heat objects around the human body, and it is easy to cause misidentification. This paper proposes to detect pedestrians in infrared images at night through the YOLO-V3 detection framework. Experimental results show that the proposed method can not only accurately locate pedestrians at night, but also maintain high detection accuracy when pedestrians overlap.},
	urldate = {2023-10-19},
	booktitle = {2021 {IEEE} 5th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	author = {Li, Wei},
	month = mar,
	year = {2021},
	note = {ISSN: 2689-6621},
	pages = {1052--1055},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\R2ZHYX36\\9390896.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\GJY85H7G\\Li - 2021 - Infrared Image Pedestrian Detection via YOLO-V3.pdf:application/pdf},
}

@inproceedings{jiangNightPedestrianDetection2022,
	title = {Night {Pedestrian} {Detection} {Method} {Based} on {Image} {Fusion}},
	url = {https://ieeexplore.ieee.org/document/10175120},
	doi = {10.1109/INSAI56792.2022.00025},
	abstract = {Improving pedestrian detection accuracy under conditions of insufficient light at night is a priority for automatic driving. The clarity of conventional visible images at night cannot be as good as that during the daytime. In contrast, the images captured by infrared cameras are almost unaffected by ambient light changes, but there are also defects of not apparent texture features. Therefore, the pedestrian characteristics can be enhanced by combining the thermal and the visible images, which are not affected by the ambient heat. There have been many types of research on pixel-level image fusion in medical diagnosis, military enhancement and navigation, and it has an excellent application value. In this paper, pixel-level fusion method is applied to the fusion of thermal and visible images, and tests on the pedestrian Dataset CVC-14 acquired in a real-world environment. The YOLOV5 method, which has significant advantages in recognition speed and application flexibility, was used for detection. The experimental results showed that the average recognition accuracy and iteration speed was superior to that of the source image. The performance of maximum fusion, principal component analysis (PCA) fusion based on thermal imaging, and weighted average fusion was outstanding. The method proposed in this paper is not complicated and widely applied, which can be improved by other researchers based on this direction.},
	urldate = {2023-10-19},
	booktitle = {2022 2nd {International} {Conference} on {Networking} {Systems} of {AI} ({INSAI})},
	author = {Jiang, Yiming and Chai, Senchun and Zhang, Baihai},
	month = oct,
	year = {2022},
	pages = {83--86},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\GGJN9B4I\\10175120.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\N9BCXYR6\\Jiang et al. - 2022 - Night Pedestrian Detection Method Based on Image F.pdf:application/pdf},
}

@inproceedings{chenMultiscaleFeatureFusion2023,
	title = {Multi-scale feature fusion pedestrian detection algorithm based on {Transformer}},
	url = {https://ieeexplore.ieee.org/document/10166718},
	doi = {10.1109/CVIDL58838.2023.10166718},
	abstract = {In this paper, a multi-scale feature fusion pedestrian detection network based on Transformer is designed for small-scale pedestrians and pedestrians disturbed by light and shadow. In the feature extraction stage, the network suppresses the interference of irrelevant features through gating mechanism and feature enhancement, enhances the discrimination of pedestrian features at different scales, dynamically controls the fusion weight of feature maps, and realizes the adaptive fusion of feature maps. In the detection stage, Transformer can capture global information and effectively solve the long-distance dependence mechanism between image pixels to improve the pedestrian detection effect. Finally, compared with the existing methods on the general pedestrian detection dataset, the average accuracy of the proposed method is 6.8 \% higher than that of the YOLOv5 model, the false detection rate is reduced by 2.7 \%, and the missed detection rate is reduced by 3.1 \%. And through the subjective evaluation of the pedestrian detection heat map, this method can detect the human body more comprehensively in the pedestrian detection task, rather than focusing on one point alone. In summary, this method can effectively improve the detection accuracy, reduce the false detection rate and missed detection rate, and improve the pedestrian detection task.},
	urldate = {2023-10-19},
	booktitle = {2023 4th {International} {Conference} on {Computer} {Vision}, {Image} and {Deep} {Learning} ({CVIDL})},
	author = {Chen, Huan and Guo, Xiaoming},
	month = may,
	year = {2023},
	pages = {536--540},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\SFULTAKY\\10166718.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\PAL852AU\\Chen and Guo - 2023 - Multi-scale feature fusion pedestrian detection al.pdf:application/pdf},
}

@inproceedings{huangPedestrianDetectionBased2022,
	title = {Pedestrian detection based on multi-scale feature fusion},
	volume = {5},
	url = {https://ieeexplore.ieee.org/document/10019992},
	doi = {10.1109/IMCEC55388.2022.10019992},
	abstract = {To solve the problem of a large difference in target size in pedestrian detection, which leads to high pedestrian false detection rate and high miss detection rate of small-scale pedestrians, a multi-scale feature fusion method based on RetinaNet is proposed. After feature enhancement by extracting features from the backbone network, three branches of different scales are formed, which are effectively fused with the corresponding feature layer to further enrich the target information, and then detect pedestrians of different scales. Test on the open dataset shows that compared with the original RetinaNet algorithm, it can detect more pedestrians, especially small-scale pedestrians, and the model detection performance is better.},
	urldate = {2023-10-19},
	booktitle = {2022 {IEEE} 5th {Advanced} {Information} {Management}, {Communicates}, {Electronic} and {Automation} {Control} {Conference} ({IMCEC})},
	author = {Huang, Lincai and Wang, Zhiwen and Fu, Xiaobiao},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-2776},
	pages = {1010--1014},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\8SIIGVZ4\\10019992.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\RJFV26PG\\Huang et al. - 2022 - Pedestrian detection based on multi-scale feature .pdf:application/pdf},
}

@inproceedings{fuPedestrianDetectionMethod2022,
	title = {Pedestrian detection method based on feature fusion {Centernet}},
	url = {https://ieeexplore.ieee.org/document/9929508},
	doi = {10.1109/IAEAC54830.2022.9929508},
	abstract = {Mainstream pedestrian detection methods require manually predefined anchor frames, and the anchor-free frame algorithm avoids the influence of manually designed anchor frames on pedestrian detection accuracy. To improve the detection performance of the pedestrian detection field, we propose a CenterNet-based feature fusion and enhanced pedestrian detection algorithm with spatial attention. Firstly, the fusion method of the backbone network feature layer and upsampled feature layer is designed to improve the utilization efficiency of feature information, redesign multiple receptive field convolution combinations to improve the feature expression ability, and finally use the spatial attention method to highlight the contribution of key information to detection. After many experiments, the present algorithm effectively improves the detection performance of the benchmark method.},
	urldate = {2023-10-19},
	booktitle = {2022 {IEEE} 6th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC} )},
	author = {Fu, Xiaobiao and Wang, Zhiwen and Huang, Lincai and Wang, Yuhang and Zhang, Canlong},
	month = oct,
	year = {2022},
	note = {ISSN: 2689-6621},
	pages = {1834--1838},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\MJJ5DJX9\\9929508.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\LLJHNDKR\\Fu et al. - 2022 - Pedestrian detection method based on feature fusio.pdf:application/pdf},
}

@inproceedings{mengMPDFFMultisourcePedestrian2022,
	title = {{MPDFF}: {Multi}-source {Pedestrian} detection based on {Feature} {Fusion}},
	shorttitle = {{MPDFF}},
	url = {https://ieeexplore.ieee.org/document/9884864},
	doi = {10.1109/IGARSS46834.2022.9884864},
	abstract = {Pedestrian detection from UAV images is vital for many fields. Given that visible images are susceptible to bad illumination, thermal images with the ability to characterize the temperature of an object can provide auxiliary information. It is useful to fuse the visible and thermal images to improve the pedestrian detection performance. Unfortunately, studies on pedestrian detection with UAV visible-thermal image pairs are still rare. Therefore, we propose a method for Multi-source Pedestrian Detection based on Feature Fusion (MPDFF). With the registered visible and thermal image pairs as the input, MPDFF can achieve better characterization of pedestrians by concatenating the features from the two images. MPDFF performs much better than the methods that use only single-source images, which demonstrates that visible and thermal images are complementary in pedestrian detection.},
	urldate = {2023-10-19},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Meng, Lingxuan and Zhou, Ji and Ma, Jin and Wang, Ziwei},
	month = jul,
	year = {2022},
	note = {ISSN: 2153-7003},
	pages = {7906--7909},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\UWKJN8ML\\9884864.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\MV5NNFCM\\Meng et al. - 2022 - MPDFF Multi-source Pedestrian detection based on .pdf:application/pdf},
}

@inproceedings{liPedestrianDetectionAlgorithm2021,
	title = {A {Pedestrian} {Detection} {Algorithm} {Based} on {Channel} {Attention} {Mechanism}},
	url = {https://ieeexplore.ieee.org/document/9601406},
	doi = {10.1109/CCDC52312.2021.9601406},
	abstract = {The main contribution of this paper is to introduce the channel attention mechanism into the feature extraction network, and propose the channel attention mechanism module CA, which realizes the efficient fusion of multi-scale features. The deformable convolution is used to replace the traditional convolution operation, and a new detection head is designed, which can predict the position of pedestrians more accurately than the original detection head. CSP is a pedestrian detection algorithm with high accuracy and fast speed, and its structure is very simple. However, there is still great potential for improvement in multi-scale feature fusion and detection head design. This paper proposes a pedestrian detection algorithm based on channel attention mechanism, which is called CA-CSP. On the basis of the original CSP algorithm, the channel attention mechanism module CA is added, and the original detection head is replaced with a detection head based on deformable convolution. The new annotation is used to evaluate the proposed pedestrian detection algorithm CA-CSP on Caltech pedestrian dataset. On the reasonable setting, using a single Nvidia 1660 GPU, CA-CSP has obtained 3.97\% of MR−2, and the original algorithm CSP has reached 4.59\% of MR−2. Compared with CSP, CA-CSP has lower MR−2. Therefore, CA-CSP has better performance than the original CSP algorithm.},
	urldate = {2023-10-19},
	booktitle = {2021 33rd {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Li, Weidong and Han, Shuang and Liu, Yang},
	month = may,
	year = {2021},
	note = {ISSN: 1948-9447},
	pages = {5954--5959},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\2RQH99GE\\9601406.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\C3XT47MS\\Li et al. - 2021 - A Pedestrian Detection Algorithm Based on Channel .pdf:application/pdf},
}

@inproceedings{fengPedestrianDetectionBased2020,
	title = {Pedestrian detection based on attention mechanism and feature enhancement with {SSD}},
	url = {https://ieeexplore.ieee.org/document/9273507},
	doi = {10.1109/CCISP51026.2020.9273507},
	abstract = {Some factors such as low resolution of small targets, limited target features, and noise interference affect the effect of pedestrian detection. Therefore, a pedestrian detection algorithm based on attention mechanism and feature enhancement with SSD is presented in this paper. It uses channel feature fusion to fuse non-adjacent convolutional layers to obtain significant edge gradient features and semantic information features.Finally, through the optimization of the attention mechanism CBAM, the channel features and spatial features are coupled under different fusion detection layers to improve the feature weight of the pedestrian's salient region, so as to the detection accuracy of the algorithm has taken a big step forward. The improved algorithm is verified on the fusion dataset and the VOC2007TEST dataset, compared with the SSD algorithm, the detection accuracy of the improved algorithm model on the fusion dataset reaches 60.1\%, with an increase of 7.2\%, and that on the VOC2007TEST dataset reaches 78\%, with an increase of 4.1\%. The experimental results show that this method can effectively detect the small target pedestrian in the image, and reduce the error detection and omission detection, thus verifying its feasibility.},
	urldate = {2023-10-19},
	booktitle = {2020 5th {International} {Conference} on {Communication}, {Image} and {Signal} {Processing} ({CCISP})},
	author = {Feng, T T and Ge, H Y},
	month = nov,
	year = {2020},
	pages = {145--148},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\MQ3F2Y39\\9273507.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\2JSSA6SP\\Feng and Ge - 2020 - Pedestrian detection based on attention mechanism .pdf:application/pdf},
}

@inproceedings{fanImprovedYOLOv5Algorithm2022,
	title = {Improved {YOLOv5} {Algorithm} {Based} on {CBAM} {Attention} {Mechanism}},
	url = {https://ieeexplore.ieee.org/document/9969224},
	doi = {10.1109/FAIML57028.2022.00051},
	abstract = {Due to the presence of tiny targets that have a high incidence of missed detection and false detection as well as the occlusion of cars and people, object recognition in road scenes is difficult. We put up a better YOLOv5 object identification model to address this problem. First, to improve the extraction of significant characteristics from cars and pedestrians while suppressing the detection of generic features, we added the CBAM attention module to the YOLOv5 backbone network. Second, we integrate two hyperparameters into the focal loss function to regulate the weight ratio of positive and negative samples and difficult and easy samples, respectively, in order to maximize the positive and negative samples in the data set and solve the issue of imbalance between difficult and easy samples. The experiment is run on the KITTI public dataset, and the mAP value is utilized as the evaluation metric. The experimental findings demonstrate that, when compared to the previous YOLOv5 model, the suggested model enhanced mAP by 1.1\%, demonstrating the new model's efficacy.},
	urldate = {2023-10-19},
	booktitle = {2022 {International} {Conference} on {Frontiers} of {Artificial} {Intelligence} and {Machine} {Learning} ({FAIML})},
	author = {Fan, Ruixiang and Qiu, Zhongpan},
	month = jun,
	year = {2022},
	pages = {229--233},
	file = {IEEE Xplore Abstract Record:H\:\\Zotero\\storage\\3DS3RLH2\\9969224.html:text/html;IEEE Xplore Full Text PDF:H\:\\Zotero\\storage\\T8XC8QEI\\Fan and Qiu - 2022 - Improved YOLOv5 Algorithm Based on CBAM Attention .pdf:application/pdf},
}

@inproceedings{vaswaniAttentionAllYou2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:H\:\\Zotero\\storage\\4L2W7JWX\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:H\:\\Zotero\\storage\\PXYL88KU\\1706.html:text/html},
}
